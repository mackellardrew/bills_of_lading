{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Shipping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I often switch between editing notebooks from my PC to my Mac;\n",
    "# the code in this cell is to find the proper local data when I do so.\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import locale\n",
    "import json\n",
    "\n",
    "o_s = platform.system()\n",
    "paths = {}\n",
    "\n",
    "if o_s == 'Darwin':\n",
    "    paths['data_dir'] = '/Users/drew/data/Data_Aggregation'\n",
    "elif o_s == 'Windows':\n",
    "    paths['data_dir'] = r'C:\\Users\\DMacKellar\\Documents\\Data\\Data_Aggregation'\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "I was recently playing with Pandas' built-in DataReader module in a project to explore some financial markets' data.  Many online examples involve using it to draw historical stock price and volume data from Yahoo or Google Finance.  Yahoo's financial data is now completely defunct (not accessible via the pandas module; there is still a [web interface](https://finance.yahoo.com/)), and a warning has been added that the Google Finance data interface is prone to failure (and, indeed, I haven't been able to recover data with it, either).  So I was perusing [the docs](https://pandas-datareader.readthedocs.io/en/latest/) associated with the DataReader module.\n",
    "\n",
    "While looking over the other data sources listed in the Pandas Datareader docs page, I saw one listed as \"the world’s largest repository of structured public data\", referring to a site called [Enigma](https://public.enigma.com/).  It does indeed have some interesting and diverse sets of data, and I quickly settled on [Bills of Lading](https://public.enigma.com/spotlight/bill-of-lading) from among the links on the front page, a set which claimed to list the contents of tons of shipping from around the world.  The top few rows listed today happened to have Seattle, WA as the port of unlading, and I thought it would be a cool practice to try to categorize all of the arrivals in Seattle for a period, and how they change over time.  [Their API](http://docs.enigma.com/public/public_v20_user_api.html) involves installing a program called Postman; I downloaded it and set it up via my Google account.  I also signed up for Enigma using my Gmail account, so it appears to have transferred the credentials automatically, because all I had to do to open the API was copy the URL from Enigma's page (after clicking on the API button within the dataset's window), paste it into the top field in the Postman window, and it returned the basic data.\n",
    "\n",
    "I'm accessing this on 20180408, and the entire Bills of Lading dataset states that it has 27,788,013 rows.  But the date range is just for 2017.  They also have separate sets for each year going back to 2014.\n",
    "\n",
    "To be specific, the overall organization of this dataset within Enigma's system is:\n",
    "\n",
    "    International Trade & Shipping\n",
    "    -> United States Import Records\n",
    "\n",
    "But within that they have 3 different sections:\n",
    "       \n",
    "       -> United States Import Records - Bill of Lading Summary\n",
    "          United States Import Records - Cargo\n",
    "          United States Import Records - Toxic & Hazardous Materials\n",
    "          \n",
    "The first two have results for each year 2014-2017, but the toxic materials data just has 2015.\n",
    "\n",
    "Summaries for each of these are offered in the \"overview\" tab:\n",
    "\n",
    "\n",
    "*Bill of Lading Summary*:\n",
    "\n",
    "Bills of lading header information for incoming shipments regulated by U.S. Customs and Border Protection's Automated Manifest System (AMS) for 2017.\n",
    "Updated a month ago\n",
    "27,788,013 rows\n",
    "31 fields\n",
    "\n",
    "*Cargo Summary*:\n",
    "\n",
    "Cargo imports in 2017. The Automated Manifest System (AMS) is designed by U.S. Customs to facilitate cargo information between steam ship lines, airlines and rail carriers for shipments destined to or transiting the United States. Recently, the processing of other cargo transportation (electronic truck, rail and sea) manifests has been transitioned to Automated Commercial Environment (ACE). AMS only refers to electronic air manifests.\n",
    "Read less\n",
    "Updated 19 days ago\n",
    "32,861,196 rows\n",
    "9 fields\n",
    "\n",
    "*Toxic & Hazardous Materials*:\n",
    "\n",
    "Information for incoming shipments containing toxic and hazardous materials provided by U.S. Customs and Border Protection's Automated Manifest System (AMS) and Automated Commercial Environment (ACE).\n",
    "Read less\n",
    "Updated 9 months ago\n",
    "\n",
    "---\n",
    "\n",
    "Actually, after poking around for a bit, using their default browser-based data viewer is pretty handy, too.  Click on the leftmost icon that appears when hovering over any column name, and [you can enter specific strings to filter its content](http://docs.enigma.com/public/public_v20_user_filtering.html).  When I enter \"Seattle, Washington\" for Port of Unlading, it returns 27,100 of 27,788,013 rows in the Bill of Lading Summary - 2017 dataset.  There's then a button to export the resulting, filtered data set as a csv file, which in this case is ~14MB.  Import below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "o_s = platform.system()\n",
    "paths = {}\n",
    "\n",
    "if o_s == 'Darwin':\n",
    "    paths['enigma_dir'] = '/Users/drew/data/Enigma'\n",
    "elif o_s == 'Windows':\n",
    "    paths['enigma_dir'] = r'C:\\Users\\DMacKellar\\Documents\\Data\\Enigma'\n",
    "\n",
    "paths['sea2017'] = os.path.join(paths['enigma_dir'], \n",
    "                                'seattle_shipping_2017.csv')\n",
    "\n",
    "sea2017_df = pd.read_csv(paths['sea2017'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>trade_update_date</th>\n",
       "      <th>run_date</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>record_status_indicator</th>\n",
       "      <th>place_of_receipt</th>\n",
       "      <th>port_of_destination</th>\n",
       "      <th>...</th>\n",
       "      <th>shipper_comm_number_qualifier</th>\n",
       "      <th>shipper_comm_number</th>\n",
       "      <th>container_number</th>\n",
       "      <th>description_sequence_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>description_text</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "      <th>harmonized_weight_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCNU3666378</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YMLU8500198</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YMLU8687140</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201712261237</td>\n",
       "      <td>2017-11-23T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>YM UNANIMITY</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-20T00:00:00</td>\n",
       "      <td>Tsingtao,China (Mainland)</td>\n",
       "      <td>New</td>\n",
       "      <td>QINGDAO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UACU5040364</td>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201712261259</td>\n",
       "      <td>2017-11-29T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan,South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>DALIAN CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YMLU8721007</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>WOODEN DOORS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     identifier    trade_update_date             run_date       vessel_name  \\\n",
       "0  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "1  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "2  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "3  201712261237  2017-11-23T00:00:00  2017-12-26T00:00:00      YM UNANIMITY   \n",
       "4  201712261259  2017-11-29T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "\n",
       "      port_of_unlading estimated_arrival_date     foreign_port_of_lading  \\\n",
       "0  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "1  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "2  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "3  Seattle, Washington    2017-12-20T00:00:00  Tsingtao,China (Mainland)   \n",
       "4  Seattle, Washington    2017-12-21T00:00:00          Pusan,South Korea   \n",
       "\n",
       "  record_status_indicator place_of_receipt port_of_destination  \\\n",
       "0                     New     NANSHA CHINA                 NaN   \n",
       "1                     New     NANSHA CHINA                 NaN   \n",
       "2                     New     NANSHA CHINA                 NaN   \n",
       "3                     New          QINGDAO                 NaN   \n",
       "4                     New     DALIAN CHINA                 NaN   \n",
       "\n",
       "           ...            shipper_comm_number_qualifier shipper_comm_number  \\\n",
       "0          ...                                      NaN                 NaN   \n",
       "1          ...                                      NaN                 NaN   \n",
       "2          ...                                      NaN                 NaN   \n",
       "3          ...                                      NaN                 NaN   \n",
       "4          ...                                      NaN                 NaN   \n",
       "\n",
       "  container_number description_sequence_number piece_count  \\\n",
       "0      TCNU3666378                           1         127   \n",
       "1      YMLU8500198                           1         136   \n",
       "2      YMLU8687140                           1          56   \n",
       "3      UACU5040364                           1        1263   \n",
       "4      YMLU8721007                           1          37   \n",
       "\n",
       "                                    description_text harmonized_number  \\\n",
       "0     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "1     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "2     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "3  PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...               NaN   \n",
       "4                                       WOODEN DOORS               NaN   \n",
       "\n",
       "  harmonized_value harmonized_weight harmonized_weight_unit  \n",
       "0              NaN               NaN                    NaN  \n",
       "1              NaN               NaN                    NaN  \n",
       "2              NaN               NaN                    NaN  \n",
       "3              NaN               NaN                    NaN  \n",
       "4              NaN               NaN                    NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea2017_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>foreign_port_of_destination</th>\n",
       "      <th>description_sequence_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.710000e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27100.000000</td>\n",
       "      <td>27100.000000</td>\n",
       "      <td>1.597600e+04</td>\n",
       "      <td>1.643600e+04</td>\n",
       "      <td>16436.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.018439e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.278155</td>\n",
       "      <td>355.122509</td>\n",
       "      <td>3.327146e+07</td>\n",
       "      <td>2.503881e+05</td>\n",
       "      <td>11218.306461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.064852e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.894217</td>\n",
       "      <td>1087.079547</td>\n",
       "      <td>4.898259e+08</td>\n",
       "      <td>7.272369e+05</td>\n",
       "      <td>28254.145238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.017123e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.013000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.017123e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.931900e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.017123e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.409100e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.017123e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>393.250000</td>\n",
       "      <td>8.422900e+05</td>\n",
       "      <td>4.243400e+05</td>\n",
       "      <td>23885.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.017123e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>101134.000000</td>\n",
       "      <td>9.404210e+09</td>\n",
       "      <td>9.748180e+06</td>\n",
       "      <td>487409.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         identifier  foreign_port_of_destination  description_sequence_number  \\\n",
       "count  2.710000e+04                          0.0                 27100.000000   \n",
       "mean   1.018439e+12                          NaN                     2.278155   \n",
       "std    9.064852e+11                          NaN                     8.894217   \n",
       "min    2.017123e+10                          NaN                     1.000000   \n",
       "25%    2.017123e+11                          NaN                     1.000000   \n",
       "50%    2.017123e+11                          NaN                     1.000000   \n",
       "75%    2.017123e+12                          NaN                     1.000000   \n",
       "max    2.017123e+12                          NaN                   133.000000   \n",
       "\n",
       "         piece_count  harmonized_number  harmonized_value  harmonized_weight  \n",
       "count   27100.000000       1.597600e+04      1.643600e+04       16436.000000  \n",
       "mean      355.122509       3.327146e+07      2.503881e+05       11218.306461  \n",
       "std      1087.079547       4.898259e+08      7.272369e+05       28254.145238  \n",
       "min         1.000000       2.013000e+04      0.000000e+00           0.000000  \n",
       "25%         8.000000       2.931900e+05      0.000000e+00           0.000000  \n",
       "50%        26.000000       4.409100e+05      0.000000e+00           0.000000  \n",
       "75%       393.250000       8.422900e+05      4.243400e+05       23885.750000  \n",
       "max    101134.000000       9.404210e+09      9.748180e+06      487409.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea2017_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Cities\n",
    "\n",
    "I'll start by just enumerating the sources of the material imported into our fair city (as of 2017, according to this data set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vancouver, BC ,Canada         13102\n",
       "Pusan,South Korea              3240\n",
       "Shanghai ,China (Mainland)     2936\n",
       "Ningpo ,China (Mainland)       1999\n",
       "Yantian,China (Mainland)        999\n",
       "Hong Kong,Hong Kong             940\n",
       "Xiamen,China (Mainland)         777\n",
       "57037                           721\n",
       "Sines,Portugal                  373\n",
       "Auckland,New Zealand            317\n",
       "Manzanillo,Mexico               266\n",
       "Cartagena ,Colombia             181\n",
       "Puerto Quetzal ,Guatemala       148\n",
       "Kaohsiung,China (Taiwan)        142\n",
       "La Spezia,Italy                 121\n",
       "Name: foreign_port_of_lading, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea2017_df['foreign_port_of_lading'].value_counts()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts of numbers of entries in the table originating in each city might be interesting (and relatively straightforward) to visualize.  First, I'll need to clean up and reformat the city names.\n",
    "\n",
    "To begin with, it's apparent from the list above that some of the ports of lading are just numerical codes.  By Googling, I find [resources](https://www.cbp.gov/sites/default/files/assets/documents/2017-Feb/appendix_f_0.pdf) that can be used to translate them.  For instance, 57037 is Yangshan, China.  I wonder how many such values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57037    721\n",
       "57073      2\n",
       "35136      1\n",
       "Name: foreign_port_of_lading, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = sea2017_df['foreign_port_of_lading'].str.match(r'[\\d.]{5}')\n",
    "sea2017_df.loc[matches, 'foreign_port_of_lading'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, just 3 unique entries.  That should be quick to fix.  Except that I can't seem to find any city corresponding to 35136.  It's probably an error in the data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_codes_dict = {'57037': 'Yangshan ,China (Mainland)', \n",
    "                     '57073': 'Nansha ,China (Mainland)',\n",
    "                     '35136': np.nan\n",
    "                    }\n",
    "\n",
    "sea2017_df['foreign_port_of_lading'].replace(origin_codes_dict, \n",
    "                                             inplace=True)\n",
    "\n",
    "# No point keeping record with no city listed\n",
    "sea2017_df = sea2017_df[sea2017_df['foreign_port_of_lading'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                     27099\n",
      "unique                       77\n",
      "top       Vancouver, BC ,Canada\n",
      "freq                      13102\n",
      "Name: foreign_port_of_lading, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sea2017_df['foreign_port_of_lading'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see where we get our imports from.  I'll want to use [plotly](https://plot.ly/python/bubble-maps/) to make a bubble map in a global mercator projection.  To get the coordinates for the cities, I'd like to try programmatic access to [Google Geocoding](https://developers.google.com/maps/documentation/geocoding/start), provided via [geopy](https://pypi.python.org/pypi/geopy/1.9.1).  One question is whether the interface is smart enough to handle the messy strings provided in the data set, or whether I'll have to clean them up, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None not found\n",
      "None not found\n",
      "中鋼, 鳳宮里, 大林蒲, 小港區, 高雄市, 812, 臺灣 22.5447429 120.356598755463\n"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as plotly\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim()\n",
    "location1 = geolocator.geocode(\"Kaohsiung,China (Taiwan)\")\n",
    "location2 = geolocator.geocode(\"Kaohsiung, China (Taiwan)\")\n",
    "location3 = geolocator.geocode(\"Kaohsiung, China\")\n",
    "locations = [location1, location2, location3]\n",
    "for location in locations:\n",
    "    if location:\n",
    "        print(location, location.latitude, location.longitude)\n",
    "    else:\n",
    "        print('{} not found'.format(location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, yeah; they need to be cleaned up.  We have cities always first, then a country, with a comma in between.  But we have some with no spaces on either side of the comma, with spaces on one side or the other, or on both sides, and none appear to be tolerated.  This definitely requires some regex.  Or maybe just '`.split()`' and '`.strip()`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vancouver, BC, Canada    13102\n",
       "Pusan, South Korea        3240\n",
       "Shanghai, China           2936\n",
       "Ningpo, China             1999\n",
       "Yantian, China             999\n",
       "Hong Kong, Hong Kong       940\n",
       "Xiamen, China              777\n",
       "Yangshan, China            721\n",
       "Sines, Portugal            373\n",
       "Auckland, New Zealand      317\n",
       "Name: lading_formatted, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "words = re.compile('[A-z]+')\n",
    "\n",
    "def get_words(source):\n",
    "    stuff = re.sub(r'\\([^)]*\\)', '', source)\n",
    "    parts = stuff.split(sep=',')\n",
    "    formatted = []\n",
    "    for part in parts:\n",
    "        new_parts = []\n",
    "        part = part.strip()\n",
    "        parts_words = re.findall(words, part)\n",
    "        new_parts.append(' '.join(parts_words))\n",
    "        formatted.append(', '.join(new_parts))\n",
    "    return str(', '.join(formatted))\n",
    "\n",
    "sea2017_df['lading_formatted'] = sea2017_df['foreign_port_of_lading']\\\n",
    ".astype(str).apply(get_words)\n",
    "\n",
    "sea2017_df['lading_formatted'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man, that took forever; I kept trying to exclude the junk in parentheses with a regex exclude clause, and couldn't get it to work.  It turns out quicker to create a new variable using `re.sub` to just get rid of anything in parentheses.  Anyways, now I want to feed them to geolocator iteratively, and build a new column with any lat/long I get out.\n",
    "\n",
    "I had some trouble with the default `Nominatim` geolocator, imported above, when trying to run the cell below.  It's specifically calling upon the [Openstreetmap server](http://geopy.readthedocs.io/en/latest/#geopy.geocoders.Nominatim), which leaves too many cities without coords.  Also, after trying to re-run them, I'm seeing timeout and other server-interaction errors.  So instead I ended up trying a couple others, and `ArcGIS` seems to work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from geopy import geocoders\n",
    "\n",
    "geolocator = geocoders.ArcGIS()\n",
    "\n",
    "cities = sea2017_df['lading_formatted'].unique()\n",
    "lat_long = dict()\n",
    "failures = []\n",
    "\n",
    "for city in cities:\n",
    "    location = geolocator.geocode(city)\n",
    "    if location:\n",
    "        lat = location.latitude\n",
    "        long = location.longitude\n",
    "        lat_long[city] = (lat, long)\n",
    "    else:\n",
    "        failures.append(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong Kong, Hong Kong', (22.351958323000076, 114.1193859870001)),\n",
       " ('Tsingtao, China', (36.098610000000065, 120.37194000000011)),\n",
       " ('Pusan, South Korea', (35.10278000000005, 129.04028000000005)),\n",
       " ('Yantian, China', (26.853330000000028, 119.85750000000007)),\n",
       " ('Shanghai, China', (31.22222000000005, 121.45806000000005)),\n",
       " ('Ningpo, China', (29.86569000000003, 121.53916000000004)),\n",
       " ('Yangshan, China', (35.18333000000007, 116.25000000000011)),\n",
       " ('Vancouver, BC, Canada', (49.260380000000055, -123.11335999999994)),\n",
       " ('Sines, Portugal', (37.957250000000045, -8.86934999999994)),\n",
       " ('Yentai, China', (37.52803000000006, 121.3826600000001))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "list(lat_long.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 0\n"
     ]
    }
   ],
   "source": [
    "print(len(lat_long.keys()), len(failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using '`geolocator2 = geocoders.GoogleV3()`' yields:\n",
    "\n",
    "    GeocoderQueryError: HTTP Error 400: Bad Request\n",
    "\n",
    "Probably something outdated about the API/geocode interface.  I tried getting an API key, which worked, but it still returns '`Bad Request`'.\n",
    "\n",
    "Anyways, I ended up trying [ArcGIS](https://developers.arcgis.com/rest/geocode/api-reference/overview-world-geocoding-service.htm), and that was sufficient to get the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(city):\n",
    "    if city in lat_long.keys():\n",
    "        coords = lat_long[city]\n",
    "    else:\n",
    "        coords = np.nan\n",
    "    return coords\n",
    "\n",
    "sea2017_df['lat_long'] = sea2017_df['lading_formatted'].astype(str).apply(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities lacking lat-long: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>trade_update_date</th>\n",
       "      <th>run_date</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>record_status_indicator</th>\n",
       "      <th>place_of_receipt</th>\n",
       "      <th>port_of_destination</th>\n",
       "      <th>...</th>\n",
       "      <th>container_number</th>\n",
       "      <th>description_sequence_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>description_text</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "      <th>harmonized_weight_unit</th>\n",
       "      <th>lading_formatted</th>\n",
       "      <th>lat_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>TCNU3666378</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8500198</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8687140</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201712261237</td>\n",
       "      <td>2017-11-23T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>YM UNANIMITY</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-20T00:00:00</td>\n",
       "      <td>Tsingtao,China (Mainland)</td>\n",
       "      <td>New</td>\n",
       "      <td>QINGDAO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>UACU5040364</td>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tsingtao, China</td>\n",
       "      <td>(36.098610000000065, 120.37194000000011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201712261259</td>\n",
       "      <td>2017-11-29T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan,South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>DALIAN CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8721007</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>WOODEN DOORS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>(35.10278000000005, 129.04028000000005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     identifier    trade_update_date             run_date       vessel_name  \\\n",
       "0  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "1  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "2  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "3  201712261237  2017-11-23T00:00:00  2017-12-26T00:00:00      YM UNANIMITY   \n",
       "4  201712261259  2017-11-29T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "\n",
       "      port_of_unlading estimated_arrival_date     foreign_port_of_lading  \\\n",
       "0  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "1  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "2  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "3  Seattle, Washington    2017-12-20T00:00:00  Tsingtao,China (Mainland)   \n",
       "4  Seattle, Washington    2017-12-21T00:00:00          Pusan,South Korea   \n",
       "\n",
       "  record_status_indicator place_of_receipt port_of_destination  \\\n",
       "0                     New     NANSHA CHINA                 NaN   \n",
       "1                     New     NANSHA CHINA                 NaN   \n",
       "2                     New     NANSHA CHINA                 NaN   \n",
       "3                     New          QINGDAO                 NaN   \n",
       "4                     New     DALIAN CHINA                 NaN   \n",
       "\n",
       "                     ...                     container_number  \\\n",
       "0                    ...                          TCNU3666378   \n",
       "1                    ...                          YMLU8500198   \n",
       "2                    ...                          YMLU8687140   \n",
       "3                    ...                          UACU5040364   \n",
       "4                    ...                          YMLU8721007   \n",
       "\n",
       "  description_sequence_number piece_count  \\\n",
       "0                           1         127   \n",
       "1                           1         136   \n",
       "2                           1          56   \n",
       "3                           1        1263   \n",
       "4                           1          37   \n",
       "\n",
       "                                    description_text harmonized_number  \\\n",
       "0     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "1     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "2     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "3  PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...               NaN   \n",
       "4                                       WOODEN DOORS               NaN   \n",
       "\n",
       "  harmonized_value harmonized_weight harmonized_weight_unit  \\\n",
       "0              NaN               NaN                    NaN   \n",
       "1              NaN               NaN                    NaN   \n",
       "2              NaN               NaN                    NaN   \n",
       "3              NaN               NaN                    NaN   \n",
       "4              NaN               NaN                    NaN   \n",
       "\n",
       "       lading_formatted                                  lat_long  \n",
       "0  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "1  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "2  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "3       Tsingtao, China  (36.098610000000065, 120.37194000000011)  \n",
       "4    Pusan, South Korea   (35.10278000000005, 129.04028000000005)  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Cities lacking lat-long: {0:.2f}%'.format(sea2017_df['lat_long'].isnull().sum() / sea2017_df.shape[1] * 100))\n",
    "sea2017_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, even though ArcGIS worked, it is finicky, and requires multiple tries, timing out often.  Since I've gotten the coordinates for the cities in a useable df, let's save this, and reload it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['sea2017_cities_coords_pkl'] = \\\n",
    "os.path.join(paths['enigma_dir'], 'sea2017_cities_coords.pkl')\n",
    "\n",
    "with open(paths['sea2017_cities_coords_pkl'], 'wb') as f:\n",
    "    pickle.dump(sea2017_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/drew/data/Enigma'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths['enigma_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~If Restarting the Notebook, Resume with Cell Below~~ (Outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I often switch between editing notebooks from my PC to my Mac;\n",
    "# the code in this cell is to find the proper local data when I do so.\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import locale\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o_s = platform.system()\n",
    "paths = {}\n",
    "\n",
    "if o_s == 'Darwin':\n",
    "    paths['data_dir'] = '/Users/drew/data/Enigma'\n",
    "elif o_s == 'Windows':\n",
    "    paths['data_dir'] = r'C:\\Users\\DMacKellar\\Documents\\Data\\\n",
    "\\Enigma'\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "for root, dirs, files in os.walk(paths['data_dir']):\n",
    "    for file in files:\n",
    "        file_under = str(file).replace('.', '_')\n",
    "        path = os.path.join(root, file)\n",
    "        paths[file_under] = path\n",
    "        \n",
    "# That should populate all data files.  Whenever a loading call fails, do:\n",
    "# print(*paths.keys(), sep='\\n')\n",
    "# And look for the key with the proper name (with underscore instead of period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>trade_update_date</th>\n",
       "      <th>run_date</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>record_status_indicator</th>\n",
       "      <th>place_of_receipt</th>\n",
       "      <th>port_of_destination</th>\n",
       "      <th>...</th>\n",
       "      <th>container_number</th>\n",
       "      <th>description_sequence_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>description_text</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "      <th>harmonized_weight_unit</th>\n",
       "      <th>lading_formatted</th>\n",
       "      <th>lat_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>TCNU3666378</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8500198</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8687140</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.351958323000076, 114.1193859870001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201712261237</td>\n",
       "      <td>2017-11-23T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>YM UNANIMITY</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-20T00:00:00</td>\n",
       "      <td>Tsingtao,China (Mainland)</td>\n",
       "      <td>New</td>\n",
       "      <td>QINGDAO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>UACU5040364</td>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tsingtao, China</td>\n",
       "      <td>(36.098610000000065, 120.37194000000011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201712261259</td>\n",
       "      <td>2017-11-29T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan,South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>DALIAN CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>YMLU8721007</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>WOODEN DOORS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>(35.10278000000005, 129.04028000000005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     identifier    trade_update_date             run_date       vessel_name  \\\n",
       "0  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "1  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "2  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "3  201712261237  2017-11-23T00:00:00  2017-12-26T00:00:00      YM UNANIMITY   \n",
       "4  201712261259  2017-11-29T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "\n",
       "      port_of_unlading estimated_arrival_date     foreign_port_of_lading  \\\n",
       "0  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "1  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "2  Seattle, Washington    2017-12-21T00:00:00        Hong Kong,Hong Kong   \n",
       "3  Seattle, Washington    2017-12-20T00:00:00  Tsingtao,China (Mainland)   \n",
       "4  Seattle, Washington    2017-12-21T00:00:00          Pusan,South Korea   \n",
       "\n",
       "  record_status_indicator place_of_receipt port_of_destination  \\\n",
       "0                     New     NANSHA CHINA                 NaN   \n",
       "1                     New     NANSHA CHINA                 NaN   \n",
       "2                     New     NANSHA CHINA                 NaN   \n",
       "3                     New          QINGDAO                 NaN   \n",
       "4                     New     DALIAN CHINA                 NaN   \n",
       "\n",
       "                     ...                     container_number  \\\n",
       "0                    ...                          TCNU3666378   \n",
       "1                    ...                          YMLU8500198   \n",
       "2                    ...                          YMLU8687140   \n",
       "3                    ...                          UACU5040364   \n",
       "4                    ...                          YMLU8721007   \n",
       "\n",
       "  description_sequence_number piece_count  \\\n",
       "0                           1         127   \n",
       "1                           1         136   \n",
       "2                           1          56   \n",
       "3                           1        1263   \n",
       "4                           1          37   \n",
       "\n",
       "                                    description_text harmonized_number  \\\n",
       "0     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "1     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "2     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "3  PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...               NaN   \n",
       "4                                       WOODEN DOORS               NaN   \n",
       "\n",
       "  harmonized_value harmonized_weight harmonized_weight_unit  \\\n",
       "0              NaN               NaN                    NaN   \n",
       "1              NaN               NaN                    NaN   \n",
       "2              NaN               NaN                    NaN   \n",
       "3              NaN               NaN                    NaN   \n",
       "4              NaN               NaN                    NaN   \n",
       "\n",
       "       lading_formatted                                  lat_long  \n",
       "0  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "1  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "2  Hong Kong, Hong Kong   (22.351958323000076, 114.1193859870001)  \n",
       "3       Tsingtao, China  (36.098610000000065, 120.37194000000011)  \n",
       "4    Pusan, South Korea   (35.10278000000005, 129.04028000000005)  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(paths['sea2017_cities_coords_pkl'], 'rb') as pickle_file:\n",
    "    sea2017_df = pickle.load(pickle_file)\n",
    "    \n",
    "sea2017_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to count the total number of containers arriving based on the city of origin, and move them into a new dataframe indexed by city name, and including the coordinates of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coords = sea2017_df['lat_long'].groupby(\n",
    "    sea2017_df['lading_formatted']).unique().rename('coords')\n",
    "\n",
    "# The lat/long are trapped in unnecessary lists: bust them out\n",
    "city_coords = city_coords.apply(lambda x: x[0])\n",
    "\n",
    "city_counts = sea2017_df['lading_formatted'].value_counts().rename('containers')\n",
    "\n",
    "city_records = pd.concat([city_coords, city_counts], axis=1)\n",
    "\n",
    "# Actually, let's just make lat and long explicit columns\n",
    "city_records['lat'] = city_records['coords'].apply(lambda x: (x)[0])\n",
    "city_records['long'] = city_records['coords'].apply(lambda x: (x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>containers</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adelaide, Australia</th>\n",
       "      <td>(-34.92584999999997, 138.5998300000001)</td>\n",
       "      <td>13</td>\n",
       "      <td>-34.925850</td>\n",
       "      <td>138.599830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Other Oman Ports, Oman</th>\n",
       "      <td>(22.993080000000077, 72.52465000000007)</td>\n",
       "      <td>1</td>\n",
       "      <td>22.993080</td>\n",
       "      <td>72.524650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anvers, Belgium</th>\n",
       "      <td>(51.22209000000004, 4.397680000000037)</td>\n",
       "      <td>6</td>\n",
       "      <td>51.222090</td>\n",
       "      <td>4.397680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auckland, New Zealand</th>\n",
       "      <td>(-36.70917666799994, 174.7330887700001)</td>\n",
       "      <td>317</td>\n",
       "      <td>-36.709177</td>\n",
       "      <td>174.733089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balboa, Panama</th>\n",
       "      <td>(8.370532224000044, -78.95069304799995)</td>\n",
       "      <td>90</td>\n",
       "      <td>8.370532</td>\n",
       "      <td>-78.950693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             coords  \\\n",
       "Adelaide, Australia         (-34.92584999999997, 138.5998300000001)   \n",
       "All Other Oman Ports, Oman  (22.993080000000077, 72.52465000000007)   \n",
       "Anvers, Belgium              (51.22209000000004, 4.397680000000037)   \n",
       "Auckland, New Zealand       (-36.70917666799994, 174.7330887700001)   \n",
       "Balboa, Panama              (8.370532224000044, -78.95069304799995)   \n",
       "\n",
       "                            containers        lat        long  \n",
       "Adelaide, Australia                 13 -34.925850  138.599830  \n",
       "All Other Oman Ports, Oman           1  22.993080   72.524650  \n",
       "Anvers, Belgium                      6  51.222090    4.397680  \n",
       "Auckland, New Zealand              317 -36.709177  174.733089  \n",
       "Balboa, Panama                      90   8.370532  -78.950693  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's try making the plot.  Since I don't know Plotly that well, I'll start by trying to import [a cell with an example from their docs](https://plot.ly/python/bubble-maps/) wholesale, then modify the most likely fields.  But I keep getting errors on the final line, referring to an error loading a URL or signing.  It appears that [Plotly wants you to get an API key](https://plot.ly/python/getting-started/#initialization-for-online-plotting), so I signed up and I'll store the key locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# DCM code for retrieving credentials\n",
    "paths['plotly_cred'] = os.path.join(paths['data_dir'], 'plotly_api_credentials.json')\n",
    "\n",
    "with open(paths['plotly_cred'], 'r') as f:\n",
    "    cred = json.load(f)\n",
    "    \n",
    "plotly_username = cred['username']\n",
    "plotly_api_key = cred['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bare-bones example to simply see if the map will render with the cities in their proper location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/40.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "\n",
    "trace = dict(type='scattergeo',\n",
    "             lon=city_records['long'].values,\n",
    "             lat=city_records['lat'].values,\n",
    "             text=city_records.index.values.tolist(),\n",
    "             mode='markers')\n",
    "py.iplot([trace])\n",
    "# print(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, cool; that was easy.  Now to adapt the tutorial's code to get the more elaborate example, where you partition the data into groups of cities based on some variable.  In this case, I'll choose based on the number of shipments they made to Seattle in 2017.  I'd like a dynamic way to do this, so I'll choose the `pd.qcut` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.0, 2.0]\n",
      "1 (2.0, 22.0]\n",
      "2 (22.0, 13102.0]\n"
     ]
    }
   ],
   "source": [
    "city_records['containers'].quantile(np.round(np.linspace(0, 1, num=4))).values.tolist()\n",
    "\n",
    "group_labels = list(((np.rint(x.left).astype(int), \n",
    "                      np.round(x.right).astype(int)) for x in pd.qcut(x=city_records['containers'], q=5, precision=0, duplicates='drop').values.unique().sort_values()))\n",
    "\n",
    "group_labels\n",
    "for i, interval in enumerate(pd.qcut(x=city_records['containers'], q=3, precision=0).values.unique().sort_values()):\n",
    "    print(i, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the process of making the graph, I encountered a lot of problems that I ended up working through piecemeal; they'll be explained in the markdown cell directly after the graph below.  But, after figuring out what I needed to do, I cleaned up this notebook, and just kept what worked.  So, for now, just accept that the next two cells are necessary.  First, I move the city names in the summary dataframe out of the index and into its own column, which gets named 'city'.  Then, I specifically write a new column containing the text that I'll want to show up as hovertext in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_records.reset_index(inplace=True)\n",
    "city_records.rename(columns={'index':'city'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_records['text'] = np.nan\n",
    "for i, row in city_records.iterrows():\n",
    "    city_records.loc[i, 'text'] = '{}: {} ship(s)'.format(row['city'], row['containers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.tools import set_credentials_file\n",
    "import pandas as pd\n",
    "\n",
    "set_credentials_file(username=plotly_username, api_key=plotly_api_key)\n",
    "\n",
    "q = 3\n",
    "city_records['quant'] = pd.qcut(\n",
    "    x=city_records['containers'], q=q, precision=0, duplicates='drop')\n",
    "\n",
    "colors = [\"rgb(0,116,217)\",\"rgb(255,65,54)\",\"rgb(133,20,75)\",\"rgb(255,133,27)\",\"lightgrey\"]\n",
    "cities = []\n",
    "scale = 3\n",
    "log_scale = 100\n",
    "\n",
    "for i, (interval, group) in enumerate(city_records.groupby('quant')):\n",
    "    lim = interval\n",
    "    city = dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'ISO-3',\n",
    "        lon = group['long'],\n",
    "        lat = group['lat'],\n",
    "        text = group['text'],\n",
    "        marker = dict(\n",
    "#             size = group['containers'] * scale, \n",
    "            size = np.log(group['containers']) * log_scale,\n",
    "            color = colors[i],\n",
    "            line = dict(width=0.5, color='rgb(40,40,40)'),\n",
    "            sizemode = 'area'\n",
    "        ),\n",
    "        name = '{:,} - {:,} ships sent'.format(\n",
    "            np.rint(lim.left).astype(int),\n",
    "            np.rint(lim.right).astype(int)) \n",
    "    )\n",
    "    cities.append(city)\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Ports of Lading for Ships <br />Arriving in Seattle in 2017',\n",
    "        showlegend = True,\n",
    "        geo = dict(\n",
    "            scope='world',\n",
    "            projection=dict( type='Mercator' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "            subunitwidth=1,\n",
    "            countrywidth=1,\n",
    "            subunitcolor=\"rgb(255, 255, 255)\",\n",
    "            countrycolor=\"rgb(255, 255, 255)\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=cities, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='sea2017_ports_of_lading' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned from First Plotly Experience\n",
    "\n",
    "Ok, this one took me a long time to figure out.  It was kind of a chain reaction of failures in adapting the code from the [plotly bubble map example](https://plot.ly/python/bubble-maps/#united-states-bubble-map) where they varied the size of city markers based on population.  At first, I tried mostly just copying their entire set of code, and substituting my dataframe's lat and lon columns for theirs, but when it came to the legend, I didn't want to just shoehorn my data into their general approach.\n",
    "\n",
    "The tutorial example used 5 \"limit\" categories, defining the quantiles of population ranks for US cities.  The thinking was obviously to make a dynamic legend that you could click on to reduce the clutter of the map to focus on specific subsets of cities: the 50th through 3000th largest cities in the US are far more numerous and far less populous than the top 50; you might want to turn off the small cities if all you care about is major metropolitan areas.  Conversely, NYC leaves an obnoxiously large imprint on the map, covering up other areas that you might want to be able to check out, so you can turn off NY and LA and see the rest of the map better, if that's your focus.\n",
    "\n",
    "But their explicit assignment of those rank intervals was a kind of inflexible approach, so I wanted to redefine the legend to be broken up into cities that send more or less stuff to Seattle on the basis of the `df.qcut` method, which breaks your rows up into quantiles of some category, then pass those to the dict-building `for` loop from the tutorial code in order to build my own custom legend.  I tried initializing the `for` loop with:\n",
    "\n",
    "    for interval, group in city_records.groupby('quant'):\n",
    "\n",
    "That worked ok for passing lat and long coords straight from the dataframe, but when I went to adapt the hover text to display the number of ships that each city sent, I ran into trouble.  I tried using:\n",
    "\n",
    "    text = '{}: {} ships'.format(group.index, group['containers'])\n",
    "    \n",
    "But whereas the `lat`, `lon`, and marker.size categories would accept arrays as input and parse them correctly, when I tried to set the text that way, it ended up passing the same array for every single row, breaking the hover text.\n",
    "\n",
    "So then I got the bright idea to break out the data within the `groupby for` loop into individual rows, using:\n",
    "\n",
    "    for interval, group in city_records.groupby('quant'):\n",
    "        for index, row in group.iterrows():\n",
    "        \n",
    "And then set every lat, lon, and text value individually with each row's unique index.  And when I ran the cell, it would execute without complaint, but the resulting map would be empty: no legend, and no city markers.  I had previously noted that the cities might not be visible if you set the marker scale too small, in which case you have to fiddle with that parameter to make them apparent, but you could still always get hovertext when you moused over the map, meaning that the cities were still being plotted.  This time, the hovertext didn't show; plotly was obviously not successfully reading the cities specified by the `for` loop.\n",
    "\n",
    "I tried outputting the `cities` list of dicts and parsing it by eye to see if it made sense, and compared the formatting to the list of dicts output by the tutorial example (of US cities, sized by population), and couldn't detect any missing fields or other dumb mistakes that would cause it to fail to plot.  I tried a lot of comparisons to no effect; the dict looked ok.  Then, finally, I happened to try checking the last dict in the tutorial example, and noticed that it looked very different from the first.  It turns out that the `for` loop from the tutorial doesn't write an individual dict for each city; it writes one dict for each legend item, or group of cities: one for each of the five groups of ranked city sizes that the author of the tutorial defined in their example.  So plotly wasn't expecting 77 dicts in the list `cities`, each with their own neatly-contained and well-defined keys and values, but rather a dict with an array of lat coords, an array of lon coords, and, for the value assigned to key 'marker', a dict containing an array of sizes corresponding to each city.\n",
    "\n",
    "I tried wrenching the nested `for` loop using `iterrows` to fit this output, but couldn't get it to work.  So I abandoned the nested loop and went back to assigning variables from just the `group`s within `groupby`, but was still stuck with the problem of not being able to dynamically assign text.  So I reverted to the approach they took in the tutorial example: they explicitly create a new column containing the data from the other columns already in the dataframe.  Then you can pass an array of previously-defined strings to plotly in a way it apparently can understand, rather than making every entry the exact same array, with too much info for each city.\n",
    "\n",
    "I had a little further trouble assigning the new `'text'` column dynamically with the iterrows function and a `'{}: {} ship(s)'.format()` call; it turns out you [can't initialize an entirely new column that way](https://stackoverflow.com/a/31460668/8637821), so you create a new column just with `np.nan` first, and use the for loop to replace them.  Finally, pandas couldn't find the city names as indices when you use iterrows in that way, and I noticed that the tutorial example just had default integer row indices, so I had to use `reset_index` to move the city names into their own column, rename the column so that it wouldn't get confused with the actual row index any more, and finally got the plot to render with the city name and number of shipments as hovertext (hence the two cells above the plot, resetting the index and specifying a new text column for each city, prior to setting up the plot itself).\n",
    "\n",
    "~~The final wrinkle for now was that, even though you could now hide Vancouver by clicking on the proper legend dot, the other cities were still too small to be visible easily on the same scale, so I adapted the size variable by calling `np.log(group['containers'])` to reduce the variation between the sizes, then, since the markers were now too small, multiplied, rather than dividing, by the scale factor.  That's a bit of a fudge, but we all make compromises when visualizing data.~~  I go back and forth on this last point; I think that having their sizes manageable to see is useful, but taking the log really makes the range of shipping less intuitive.  You can comment out either of the above `size` parameters in the `markers` dict in order to see either approach.\n",
    "\n",
    "Note, also, that you can dynamically alter the graph by changing the number of quantiles set by the first arg, '`q`'.  But the `qcut` method seeks to make all the bins of the same size, and since the most cities send very very few ships, you end up with multiple quantiles that contain just one or two records, and since the intervals must be unique, it can't display them separately, meaning you can't get all that many quantiles to display this way.  You can avoid getting warnings for overlapping quantiles by calling the arg `drop_duplicates=True`, but it'll still plot fewer than you call for.  There may be a way to skew the quantiles to preferentially break up the groups that contain more ships, but I haven't found a way to do this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Against Other Cities/Years\n",
    "\n",
    "Now, the task is to take what I've learned, and generalize the pipeline so I can easily feed in different time ranges or cities of destination in order to do comparisons with others from the Enigma bills of lading dataset.  I think a good workflow would be to have a few cells that could accept a csv file containing a subset of the Enigma db's shipping data from any given year, focused on the city of destination, and then reformat the city names, retrieve coordinates, and map them by number of containers shipped, as I've done above for Seattle in 2017.  It would be cool to have a more programmatic interface to cut out some of the human-centric downloading and moving files around, and their [API](http://docs.enigma.com/public/public_v20_user_pandas.html#api_export) should ultimately be compatible with this, but for now I'll settle with just having the work I did above to map one city's imports extrapolate to others in a matter of minutes.\n",
    "\n",
    "Let's start with comparing Seattle's imports to that of a few other ports.  According to the frequency rank view of the summary statistics for the [Bill of Lading Summary db in Enigma for 2017](https://public.enigma.com/datasets/0293cd20-8580-4d30-b173-2ac27952b74b), the busiest ports that year were:\n",
    "\n",
    "    Long Beach, California:   118,493 records\n",
    "    Los Angeles, California:  105,329\n",
    "    New York/Newark, NJ:       94,620\n",
    "    Houston, Texas:            39,555\n",
    "    Tacoma, Washington:        37,106\n",
    "    \n",
    "I had previously heard from a friend that Long Beach was the busiest US port for cargo traffic, but didn't know that LA proper has almost as much total activity; together, SoCal is definitely the hub for import in the US.  Coming in at a close third is no surprise: NYC has been the commercial hub for the country for a long time, although they moved the bulk of traffic from being processed in Manhattan [over to New Jersey in the mid-20th century](https://en.wikipedia.org/wiki/New_York_Harbor#Container_shipping_and_air_travel).  It's interesting that Houston is next; I hadn't heard of them being a particularly big depot, although perhaps it makes sense, as it's geographically closer to the center of the continental states than New York or LA.  Finally, I had also previously heard that Tacoma is larger than Seattle when it comes to cargo traffic.\n",
    "\n",
    "That's a good cross-section of major commercial traffic in the US, let's try plotting them and seeing whether the city of origin for imports varies based on their destination among US ports.  I'll import the data, reformat the city names, download coords, and try to get them to plot in different subplots with plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a list of files to be opened, processed, and plotted\n",
    "cities_2017_paths = {'Long_Beach': paths['longbeach_shipping_2017_csv'],\n",
    "                     'Los_Angeles': paths['losangeles_shipping_2017_csv'],\n",
    "                     'NYC_Newark': paths['nyc_shipping_2017_csv'],\n",
    "                     'Houston': paths['houston_shipping_2017_csv'],\n",
    "                     'Tacoma': paths['tacoma_shipping_2017_csv'],\n",
    "                     'Seattle': paths['seattle_shipping_2017_csv']}\n",
    "cities_2017_dfs = {}\n",
    "\n",
    "# Import each into Pandas\n",
    "for city, path in cities_2017_paths.items():\n",
    "    cities_2017_dfs[city] = pd.read_csv(path)\n",
    "    # Add another level to column names to keep grouped together by city\n",
    "#     cities_2017_dfs[city].columns = \\\n",
    "#     pd.MultiIndex.from_product([cities_2017_dfs[city].columns, [city]])\n",
    "    # Combine into one df\n",
    "#     cities_2017_df = pd.merge(cities_2017_dfs.values(), how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "Long_Beach:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "Los_Angeles:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "NYC_Newark:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "Seattle:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "Tacoma:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for ports of lading with numeric code, rather than names\n",
    "def code_lookup(dict_of_dfs):\n",
    "    matches_set = set()\n",
    "    for city, df in dict_of_dfs.items():\n",
    "        matches = df['foreign_port_of_lading'].str.match(r'[\\d]+')\n",
    "        matches_series = df.loc[matches, \n",
    "                                'foreign_port_of_lading'].value_counts()\n",
    "        matches_set.update(matches_series.index.values)\n",
    "        print('{}:\\n{}'.format(city, matches_series))\n",
    "    return matches_set\n",
    "\n",
    "code_matches = code_lookup(cities_2017_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated City Lookup?\n",
    "\n",
    "An aside here: I the list of numerically-coded ports of lading above is manageable, and I could easily create a dict to replace them with the appropriate city, just by reading through [this document](https://www.cbp.gov/sites/default/files/assets/documents/2017-Feb/appendix_f_0.pdf).  But to ensure that future, unanticipated codes could be dealt with automatically, I wondered if it would be possible to just parse the PDF that contains those codes, and automatically generate code: city pairs.\n",
    "\n",
    "I searched for modules for extracting text from PDFs, and heard to try `textract`.  When pip installing it, however, it returns an error because one of its dependencies, `EBookLib`, has a readme file within its pip wheel that contains non-standard characters, that cause it to throw an error.  I found a [thread on this](https://github.com/deanmalmgren/textract/issues/194), and it sounded do-able.  I opened the offending file in Notepad++, replaced the string, and saved, but after clearing the pip cache and re-running, I hit the same error.  The helpful author had [another suggestion](https://github.com/deanmalmgren/textract/issues/194#issuecomment-357416217) about editing the file in pip, but while that code executed, the same error results when trying `pip install textract`.  I verified that the README file had been successfully modified to remove the characters in question, but it just never worked.\n",
    "\n",
    "I finally got the pip install to work by just skipping to the end of the thread and running [this command](https://github.com/deanmalmgren/textract/issues/194#issuecomment-378054308).  I probably spent a couple of hours just trying to get this stupid module.  I'll try it below and see if it's suitable for automating the code-name replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9ad9b588f2f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.cbp.gov/sites/default/files/assets/documents/2017-Feb/appendix_f_0.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'codes_pdf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'codes.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textract'"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "import requests\n",
    "\n",
    "url = 'https://www.cbp.gov/sites/default/files/assets/\\\n",
    "documents/2017-Feb/appendix_f_0.pdf'\n",
    "paths['codes_pdf'] = os.path.join(paths['data_dir'], 'codes.pdf')\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(paths['codes_pdf'], 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "pdf_file = open(paths['codes_pdf'], 'rb')\n",
    "pdf_data = textract.process(paths['codes_pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf_data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's not as clean as I would like, but a pattern is visible.  The country names are preceded by '\\r\\n\\r\\n', then the name ('Albania'), then there's just '\\r\\n' between all of the codes, then there's another '\\r\\n\\r\\n' between the last code and the first city name, then there's another '\\r\\n\\r\\n' before the next country name ('Algeria'), and the pattern repeats.  In other words, there is [one ASCII carriage return and one 'linefeed'](https://docs.python.org/2.0/ref/strings.html) between each country name and the first code, between each adjacent pair of codes, and between each city name, but there are two of each of those breaks between the last city name and the next country, as well as between the last code and the first city name.  That isn't **too** complicated, but I was hoping that the text would be interpreted as a more direct spatial coordination between code and city name.\n",
    "\n",
    "I definitely could write a custom function that would parse this, but I've already spent enough time on this.  For the purposes of this notebook, I'll just create a dict manually to handle the missing city names from the dataframes above.  Unfortunately, many of the codes couldn't be found, either on the PDF listed above or a couple other references.  The codes are consistent enough to tell what country they're referring to, but the cities are missing, and I choose not to guess by picking the closest integer number.\n",
    "\n",
    "I won't mess around with more annotation here; I'll just write one function to parse the dfs up to the point of obtaining their coords, then save the results as pickle files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidating Data Aggregation/Cleanup; Applying to Other Ports of Unlading\n",
    "\n",
    "I'm going to try to recapitulate the process I used above to get the lat-long coords for ports of lading to Seattle, and extend it as smoothly as possible to the other cities.  I'll see how succinctly I can achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a list of files to be opened, processed, and plotted\n",
    "cities_2017_paths = {'Long_Beach': paths['longbeach_shipping_2017_csv'],\n",
    "                     'Los_Angeles': paths['losangeles_shipping_2017_csv'],\n",
    "                     'NYC_Newark': paths['nyc_shipping_2017_csv'],\n",
    "                     'Houston': paths['houston_shipping_2017_csv'],\n",
    "                     'Tacoma': paths['tacoma_shipping_2017_csv'],\n",
    "                     'Seattle': paths['seattle_shipping_2017_csv']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# DCM code for retrieving credentials\n",
    "paths['plotly_cred'] = os.path.join(paths['data_dir'], 'plotly_api_credentials.json')\n",
    "paths['google_cred'] = os.path.join(paths['data_dir'], 'google_api_cred.json')\n",
    "\n",
    "with open(paths['plotly_cred'], 'r') as f:\n",
    "    plotly_cred = json.load(f)\n",
    "with open(paths['google_cred'], 'r') as f:\n",
    "    google_cred = json.load(f)\n",
    "    \n",
    "plotly_username = plotly_cred['username']\n",
    "plotly_api_key = plotly_cred['api_key']\n",
    "google_api_key = google_cred['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from geopy import geocoders\n",
    "\n",
    "# Specify the dict of codes to replace with city names\n",
    "origin_codes_dict = {'57073': 'Nansha, China (Mainland)', \n",
    "                     '53304': 'Pipavav, India', \n",
    "                     '30110': 'Covenas, Columbia', \n",
    "                     '57037': 'Yangshan, China (Mainland)'\n",
    "                    }\n",
    "\n",
    "\n",
    "def import_shipping_data(paths):\n",
    "    cities_dfs = {}\n",
    "    # Import each port of unlading's shipping records into Pandas\n",
    "    for city, path in paths.items():\n",
    "        cities_dfs[city] = pd.read_csv(path)\n",
    "    return cities_dfs\n",
    "\n",
    "\n",
    "def replace_city_codes(cities_dfs, codes_dict):\n",
    "    # Initialize counters to print how many records were dropped\n",
    "    records_dropped = 0\n",
    "    total_records = 0\n",
    "    replaced = {}\n",
    "    # First, see if any 5-digit codes present in dfs\n",
    "    # were missing from dict; if so, add to dict as NaN\n",
    "    for city, df in cities_dfs.items():\n",
    "        matches = df.loc[:,'foreign_port_of_lading'].str.match(r'[\\d.]{5}')\n",
    "        match_list = df.loc[matches, \n",
    "                            'foreign_port_of_lading'].unique().tolist()\n",
    "        for match in match_list:\n",
    "            if match not in codes_dict:\n",
    "                codes_dict[match] = np.nan\n",
    "                print('\"{}\" not found in input dict; \\\n",
    "adding to dict as np.nan.'.format(match))\n",
    "            else:\n",
    "                pass\n",
    "        # Replace codes with cities where possible; drop the rest\n",
    "        new_df = df.copy()\n",
    "        new_df.loc[:,'foreign_port_of_lading'].replace(codes_dict, \n",
    "                                                       inplace=True)\n",
    "        # No point keeping record with no city listed\n",
    "        records_dropped += new_df['foreign_port_of_lading'].isnull().sum()\n",
    "        total_records += new_df.shape[0]\n",
    "        new_df = new_df[new_df.loc[:,'foreign_port_of_lading'].notnull()]\n",
    "        replaced[city] = new_df\n",
    "    # Add a print line to summarize how many records were discarded\n",
    "    print('\\n{} records were dropped from bills of lading \\\n",
    "because no foreign port was found with the numeric code given; \\\n",
    "amounting to {:.8f}% of all records.'.format(\n",
    "    records_dropped, records_dropped*100/total_records))\n",
    "    return replaced\n",
    "\n",
    "\n",
    "def get_words(source):\n",
    "    words = re.compile('[A-z]+')\n",
    "    # Get rid of '(Mainland)' and other parenthetical stuff\n",
    "    noparens = re.sub(r'\\([^)]*\\)', '', source)\n",
    "    # Separate city from country\n",
    "    parts = noparens.split(sep=',')\n",
    "    formatted = []\n",
    "    # Process city and country separately\n",
    "    for part in parts:\n",
    "        new_parts = []\n",
    "        # Get rid of leading/trailing whitespace\n",
    "        part = part.strip()\n",
    "        # Discard punctuation, numbers; any non-words\n",
    "        parts_words = re.findall(words, part)\n",
    "        # Separate individual words by a single space\n",
    "        new_parts.append(' '.join(parts_words))\n",
    "        # Re-join city and country, separated by comma and space\n",
    "        formatted.append(', '.join(new_parts))\n",
    "    return str(', '.join(formatted))\n",
    "\n",
    "\n",
    "def format_ports(cities_dfs):\n",
    "    new_port_dfs = {}\n",
    "    # Use pandas' vectorized function, \n",
    "    # rather than iterating over rows slowly\n",
    "    for city, df in cities_dfs.items():\n",
    "        df.loc[:,'foreign_port_of_lading'] = df.loc[:,\n",
    "            'foreign_port_of_lading'].astype(str).apply(get_words)\n",
    "        new_port_dfs[city] = df\n",
    "    return new_port_dfs\n",
    "\n",
    "\n",
    "def geolocate(cities_dfs):\n",
    "    # ArcGIS no longer tolerates me; use Google\n",
    "    geolocator = geocoders.GoogleV3(api_key=google_api_key,\n",
    "                                    timeout=100)\n",
    "    all_foreign = set()\n",
    "    lat_long = dict()\n",
    "    failures = []\n",
    "    # Don't spam the server with redundant calls;\n",
    "    # first gather all unique cities together\n",
    "    for city, df in cities_dfs.items():\n",
    "        all_foreign.update(\n",
    "            df.loc[:,'foreign_port_of_lading'].unique().astype(str))\n",
    "    # Feed all unique cities to server, save output\n",
    "    # Either in dict with coords, or else as failures\n",
    "    for port in all_foreign:\n",
    "        location = geolocator.geocode(port)\n",
    "        if location:\n",
    "            lat = location.latitude\n",
    "            long = location.longitude\n",
    "            lat_long[port] = (lat, long)\n",
    "        else:\n",
    "            failures.append(port)\n",
    "    return lat_long, failures\n",
    "\n",
    "            \n",
    "def lookup(port, lat_long_dict):\n",
    "    if port in lat_long_dict.keys():\n",
    "        coords = lat_long_dict[port]\n",
    "    else:\n",
    "        coords = np.nan\n",
    "    return coords\n",
    "\n",
    "\n",
    "def get_coords(cities_dfs, lat_long_dict):\n",
    "    coordinated = {}\n",
    "    # Again, use vectorized functions\n",
    "    for city, df in cities_dfs.items():\n",
    "        new_df = df.copy()\n",
    "        new_df['lat'] = np.nan\n",
    "        new_df['long'] = np.nan\n",
    "        new_df['lat_long'] = np.nan\n",
    "        new_df['lat_long'] = new_df['foreign_port_of_lading'].astype(\n",
    "            str).apply(lookup, lat_long_dict=lat_long_dict)\n",
    "        lat_dict = {}\n",
    "        long_dict = {}\n",
    "        for port, coords in lat_long_dict.items():\n",
    "            lat_dict[port] = coords[0]\n",
    "            long_dict[port] = coords[1]\n",
    "        new_df['lat'] = new_df['foreign_port_of_lading'].astype(\n",
    "            str).apply(lookup, lat_long_dict=lat_dict)\n",
    "        new_df['long'] = new_df['foreign_port_of_lading'].astype(\n",
    "            str).apply(lookup, lat_long_dict=long_dict)\n",
    "        # drop any without coords\n",
    "        empties = new_df.loc[:,'lat_long'].isnull().sum()\n",
    "        print('{:<12}\\nCoordinates couldn\\'t be found for {:>5} \\\n",
    "ports of lading; {:>.2f}% of all records'.format(\n",
    "            city, empties, empties/new_df.shape[0]*100))\n",
    "        new_df.dropna(subset=['lat_long'], inplace=True)\n",
    "        coordinated[city] = new_df\n",
    "    return coordinated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took me significantly longer than I expected to refactor the approach that worked above for Seattle into a series of functions that would make for a straightforward pipeline to modify the dataframes for several ports of unlading.  I still don't have a lot of experience writing extended pipelines of functions; I could've left a lot of the code the same, but wanted to be able to invoke the whole process in as few lines as possible.\n",
    "\n",
    "I ended up having to write two new functions (`format_ports` and `get_coords`) to replace the `.apply(custom_func)` calls that I used in the original case to both reformat the `foreign_port_of_lading` strings and then to populate the `lat_long` column from the dict.  I could've left those calls separate but, again, I wanted to simplify.  I could also still write a single wrapper function to reduce the 5 lines in the cell below with a single call, but I found it useful to keep them separate while troubleshooting the various functions.  Now that they work I'll still keep them separate.\n",
    "\n",
    "I'm aware that defining a single class, with each of these as methods within, is probably the canonical, Pythonic way to go, but I have even less experience at present with proper class coding, so I'll pass on that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long_Beach  \n",
      "Coordinates couldn't be found for  1762 ports of lading; 1.49% of all records\n",
      "Los_Angeles \n",
      "Coordinates couldn't be found for  1312 ports of lading; 1.25% of all records\n",
      "NYC_Newark  \n",
      "Coordinates couldn't be found for    71 ports of lading; 0.08% of all records\n",
      "Houston     \n",
      "Coordinates couldn't be found for    54 ports of lading; 0.14% of all records\n",
      "Tacoma      \n",
      "Coordinates couldn't be found for  3186 ports of lading; 8.59% of all records\n",
      "Seattle     \n",
      "Coordinates couldn't be found for     1 ports of lading; 0.00% of all records\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cities_2017_dfs = import_shipping_data(cities_2017_paths)\n",
    "cities_2017_dfs = replace_city_codes(cities_2017_dfs, origin_codes_dict)\n",
    "cities_2017_dfs = format_ports(cities_2017_dfs)\n",
    "# lat_long, failures = geolocate(cities_2017_dfs)\n",
    "cities_2017_dfs = get_coords(cities_2017_dfs, lat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Long_Beach', 'Los_Angeles', 'NYC_Newark', 'Houston', 'Tacoma', 'Seattle'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_2017_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>trade_update_date</th>\n",
       "      <th>run_date</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>record_status_indicator</th>\n",
       "      <th>place_of_receipt</th>\n",
       "      <th>port_of_destination</th>\n",
       "      <th>...</th>\n",
       "      <th>description_sequence_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>description_text</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "      <th>harmonized_weight_unit</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>lat_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.396428</td>\n",
       "      <td>114.109497</td>\n",
       "      <td>(22.396428, 114.109497)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.396428</td>\n",
       "      <td>114.109497</td>\n",
       "      <td>(22.396428, 114.109497)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201712261102</td>\n",
       "      <td>2017-11-22T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>New</td>\n",
       "      <td>NANSHA CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>SOFA &amp; UPHOLSTERY PO418287/418285/418967 \"SH \"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.396428</td>\n",
       "      <td>114.109497</td>\n",
       "      <td>(22.396428, 114.109497)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201712261237</td>\n",
       "      <td>2017-11-23T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>YM UNANIMITY</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-20T00:00:00</td>\n",
       "      <td>Tsingtao, China</td>\n",
       "      <td>New</td>\n",
       "      <td>QINGDAO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.067108</td>\n",
       "      <td>120.382609</td>\n",
       "      <td>(36.067108, 120.382609)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201712261259</td>\n",
       "      <td>2017-11-29T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>DALIAN CHINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>WOODEN DOORS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.179554</td>\n",
       "      <td>129.075642</td>\n",
       "      <td>(35.1795543, 129.0756416)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     identifier    trade_update_date             run_date       vessel_name  \\\n",
       "0  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "1  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "2  201712261102  2017-11-22T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "3  201712261237  2017-11-23T00:00:00  2017-12-26T00:00:00      YM UNANIMITY   \n",
       "4  201712261259  2017-11-29T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "\n",
       "      port_of_unlading estimated_arrival_date foreign_port_of_lading  \\\n",
       "0  Seattle, Washington    2017-12-21T00:00:00   Hong Kong, Hong Kong   \n",
       "1  Seattle, Washington    2017-12-21T00:00:00   Hong Kong, Hong Kong   \n",
       "2  Seattle, Washington    2017-12-21T00:00:00   Hong Kong, Hong Kong   \n",
       "3  Seattle, Washington    2017-12-20T00:00:00        Tsingtao, China   \n",
       "4  Seattle, Washington    2017-12-21T00:00:00     Pusan, South Korea   \n",
       "\n",
       "  record_status_indicator place_of_receipt port_of_destination  \\\n",
       "0                     New     NANSHA CHINA                 NaN   \n",
       "1                     New     NANSHA CHINA                 NaN   \n",
       "2                     New     NANSHA CHINA                 NaN   \n",
       "3                     New          QINGDAO                 NaN   \n",
       "4                     New     DALIAN CHINA                 NaN   \n",
       "\n",
       "             ...              description_sequence_number piece_count  \\\n",
       "0            ...                                        1         127   \n",
       "1            ...                                        1         136   \n",
       "2            ...                                        1          56   \n",
       "3            ...                                        1        1263   \n",
       "4            ...                                        1          37   \n",
       "\n",
       "                                    description_text harmonized_number  \\\n",
       "0     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "1     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "2     SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"               NaN   \n",
       "3  PE TARPAULIN THIS SHIPMENT CONTAINS NO WOOD P ...               NaN   \n",
       "4                                       WOODEN DOORS               NaN   \n",
       "\n",
       "  harmonized_value harmonized_weight harmonized_weight_unit        lat  \\\n",
       "0              NaN               NaN                    NaN  22.396428   \n",
       "1              NaN               NaN                    NaN  22.396428   \n",
       "2              NaN               NaN                    NaN  22.396428   \n",
       "3              NaN               NaN                    NaN  36.067108   \n",
       "4              NaN               NaN                    NaN  35.179554   \n",
       "\n",
       "         long                   lat_long  \n",
       "0  114.109497    (22.396428, 114.109497)  \n",
       "1  114.109497    (22.396428, 114.109497)  \n",
       "2  114.109497    (22.396428, 114.109497)  \n",
       "3  120.382609    (36.067108, 120.382609)  \n",
       "4  129.075642  (35.1795543, 129.0756416)  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_2017_dfs['Seattle'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too long a time to wait, considering the amount of data being processed.  Most of it was in the lookup time for retrieving the coordinates from Google using `geolocate`.  Note that, while I used ArcGIS for the Seattle df earlier in this notebook, it repeatedly refused to process so many ports of origin this time around.  It [sounds like](https://developers.arcgis.com/rest/geocode/api-reference/geocoding-free-vs-paid.htm) they aren't expecting you to do bulk access of their geolocator db, maybe just a few records at a time.  So I switched to [GoogleV3](https://developers.google.com/maps/documentation/geocoding/start), got an API key, and they were more patient with me (at least once I changed the default `timeout=1` arg upon initialization).  Let's see how many coords are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, df in cities_2017_dfs.items():\n",
    "    if df['lat_long'].isnull().sum() != 0:\n",
    "        print('{:<15}{:.6f}'.format(\n",
    "            city, df['lat_long'].isnull().sum()/df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   North Pacific, United States Off Shore Tanker Transshipment Points\n",
      "1   High Seas, United States Off Shore Tanker Transshipment Points\n",
      "2   All Other Egypt Mediterranean Region Ports, Egypt\n",
      "3   Hitaki Ko, Japan\n",
      "4   All Other Netherlands Antilles Ports, Netherland Antilles\n",
      "5   Khor al Ami, Kuwait\n",
      "6   Nagoya Ko, Japan\n",
      "7   Puduchcheri, India\n",
      "8   Gulf of Mexico, United States Off Shore Tanker Transshipment Points\n",
      "9   Port Redon, Vietnam\n",
      "10  All Other Estonia Ports, Estonia\n",
      "11  Soyo Quinfuquena term, Angola\n"
     ]
    }
   ],
   "source": [
    "for i, failure in enumerate(failures):\n",
    "    print('{:<4}{}'.format(i, failure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there were significantly more records lost for destinations other than Seattle because Google didn't find the coordinates of the port of lading.  But from looking at the list of the `failures`, I can't blame Google at all; those are some strange, wonky-sounding origins in several cases.  The phenomenon of \"transshipment\" [throws a considerable monkey wrench into things](https://www.usitc.gov/publications/332/ec200404b.pdf), and I'm not gonna solve that issue here.\n",
    "\n",
    "Okay!  Now I'm ready to plot the various ports of lading for these respective US ports of unlading.  I'll drop the rows lacking coord data; I don't expect attempting to cover the exceptions found in `failures` will prove very edifying, and they still collectively represent around 1% or less for each of the cities (except Tacoma; and it could be interesting to look into why someday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long_Beach   size before dropna: \t118483\n",
      "Long_Beach   size after dropna: \t116721\n",
      "Los_Angeles  size before dropna: \t105320\n",
      "Los_Angeles  size after dropna: \t104008\n",
      "NYC_Newark   size before dropna: \t94219\n",
      "NYC_Newark   size after dropna: \t94148\n",
      "Houston      size before dropna: \t39071\n",
      "Houston      size after dropna: \t39017\n",
      "Tacoma       size before dropna: \t37106\n",
      "Tacoma       size after dropna: \t33920\n",
      "Seattle      size before dropna: \t27099\n",
      "Seattle      size after dropna: \t27098\n"
     ]
    }
   ],
   "source": [
    "cities_2017_dfs_clean = {}\n",
    "for city, df in cities_2017_dfs.items():\n",
    "    print('{:<12} size before dropna: \\t{}'.format(city, df.shape[0]))\n",
    "    cities_2017_dfs_clean[city] = df[df['lat_long'].notnull()]\n",
    "    print('{:<12} size after dropna: \\t{}'.format(\n",
    "        city, cities_2017_dfs_clean[city].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now I'll just want to pickle these (as well as the lat_long dict), so I can access them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities_2017_dfs.keys():\n",
    "    name = '{}_2017_pkl'.format(city)\n",
    "    filename = '{}_2017.pkl'.format(city)\n",
    "    paths[name] = os.path.join(paths['data_dir'], filename)\n",
    "    \n",
    "for city, df in cities_2017_dfs_clean.items():\n",
    "    name = '{}_2017_pkl'.format(city)\n",
    "    with open(paths[name], 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        \n",
    "paths['lat_long_dict'] = os.path.join(paths['data_dir'], \n",
    "                                      'lat_long_dict.pkl')\n",
    "with open(paths['lat_long_dict'], 'wb') as f:\n",
    "    pickle.dump(lat_long, f)\n",
    "    \n",
    "paths['unlading_coords'] = os.path.join(paths['data_dir'], 'top_6_unlading.pkl')\n",
    "\n",
    "with open(paths['unlading_coords'], 'wb') as f:\n",
    "    pickle.dump(unlading_coords, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If Resuming Notebook, Start With Cell Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I often switch between editing notebooks from my PC to my Mac;\n",
    "# the code in this cell is to find the proper local data when I do so.\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import locale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "o_s = platform.system()\n",
    "paths = {}\n",
    "\n",
    "if o_s == 'Darwin':\n",
    "    paths['data_dir'] = '/Users/drew/data/Enigma'\n",
    "elif o_s == 'Windows':\n",
    "    paths['data_dir'] = r'C:\\Users\\DMacKellar\\Documents\\Data\\\n",
    "\\Enigma'\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "# DCM code for retrieving credentials\n",
    "paths['plotly_cred'] = os.path.join(paths['data_dir'], 'plotly_api_credentials.json')\n",
    "paths['google_cred'] = os.path.join(paths['data_dir'], 'google_api_cred.json')\n",
    "\n",
    "with open(paths['plotly_cred'], 'r') as f:\n",
    "    plotly_cred = json.load(f)\n",
    "with open(paths['google_cred'], 'r') as f:\n",
    "    google_cred = json.load(f)\n",
    "    \n",
    "plotly_username = plotly_cred['username']\n",
    "plotly_api_key = plotly_cred['api_key']\n",
    "google_api_key = google_cred['api_key']\n",
    "\n",
    "for root, dirs, files in os.walk(paths['data_dir']):\n",
    "    for file in files:\n",
    "        file_under = str(file).replace('.', '_')\n",
    "        path = os.path.join(root, file)\n",
    "        paths[file_under] = path\n",
    "        \n",
    "# That should populate all data files.  Whenever a loading call fails, do:\n",
    "# print(*paths.keys(), sep='\\n')\n",
    "# And look for the key with the proper name (with underscore instead of period)\n",
    "\n",
    "# For now, just load the 6 dfs with lat_long coords\n",
    "cities_2017_dfs = {}\n",
    "for path in paths:\n",
    "    if path[-9:] == '_2017_pkl':\n",
    "        with open(paths[path], 'rb') as f:\n",
    "            name = str(path[:-9])\n",
    "            cities_2017_dfs[name] = pickle.load(f)\n",
    "\n",
    "# Load api credentials\n",
    "with open(paths['credentials_json'], 'r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each port of unlading, aggregate ports of lading\n",
    "def get_counts(dict_of_dfs):\n",
    "    city_counts_dfs = {}\n",
    "\n",
    "    for city, df in dict_of_dfs.items():\n",
    "        # Getting tired of typing out 'foreign_port_of_lading' all the time\n",
    "        df1 = df.copy()\n",
    "        df1.rename(columns={'foreign_port_of_lading':'city'}, inplace=True)\n",
    "        grouped = df1.groupby(df['city'])\n",
    "        containers = grouped.size().to_dict()\n",
    "        df1['containers'] = df1.loc[:, 'city'\n",
    "                                 ].apply(lambda x: containers[x])\n",
    "        cols = ['city', 'lat_long', \n",
    "                'lat', 'long', 'containers']\n",
    "        unique = df1.drop_duplicates(subset=['city'])\n",
    "        city_records = unique.loc[:, cols]\n",
    "\n",
    "        # Make a column to contain the 'text' data you want in hovertext:\n",
    "        city_records['text'] = np.nan\n",
    "        for i, row in city_records.iterrows():\n",
    "            city_records.loc[i, 'text'] = '{}: {} container(s)'.format(\n",
    "                row['city'], row['containers'])\n",
    "        city_counts_dfs[city] = city_records\n",
    "    return city_counts_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_counts_dfs = get_counts(cities_2017_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>containers</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.396428, 114.109497)</td>\n",
       "      <td>22.396428</td>\n",
       "      <td>114.109497</td>\n",
       "      <td>940</td>\n",
       "      <td>Hong Kong, Hong Kong: 940 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tsingtao, China</td>\n",
       "      <td>(36.067108, 120.382609)</td>\n",
       "      <td>36.067108</td>\n",
       "      <td>120.382609</td>\n",
       "      <td>23</td>\n",
       "      <td>Tsingtao, China: 23 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>(35.1795543, 129.0756416)</td>\n",
       "      <td>35.179554</td>\n",
       "      <td>129.075642</td>\n",
       "      <td>3240</td>\n",
       "      <td>Pusan, South Korea: 3240 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yantian, China</td>\n",
       "      <td>(22.556499, 114.236875)</td>\n",
       "      <td>22.556499</td>\n",
       "      <td>114.236875</td>\n",
       "      <td>999</td>\n",
       "      <td>Yantian, China: 999 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shanghai, China</td>\n",
       "      <td>(31.2303904, 121.4737021)</td>\n",
       "      <td>31.230390</td>\n",
       "      <td>121.473702</td>\n",
       "      <td>2936</td>\n",
       "      <td>Shanghai, China: 2936 container(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    city                   lat_long        lat        long  \\\n",
       "0   Hong Kong, Hong Kong    (22.396428, 114.109497)  22.396428  114.109497   \n",
       "3        Tsingtao, China    (36.067108, 120.382609)  36.067108  120.382609   \n",
       "4     Pusan, South Korea  (35.1795543, 129.0756416)  35.179554  129.075642   \n",
       "5         Yantian, China    (22.556499, 114.236875)  22.556499  114.236875   \n",
       "11       Shanghai, China  (31.2303904, 121.4737021)  31.230390  121.473702   \n",
       "\n",
       "    containers                                    text  \n",
       "0          940  Hong Kong, Hong Kong: 940 container(s)  \n",
       "3           23        Tsingtao, China: 23 container(s)  \n",
       "4         3240   Pusan, South Korea: 3240 container(s)  \n",
       "5          999        Yantian, China: 999 container(s)  \n",
       "11        2936      Shanghai, China: 2936 container(s)  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_counts_dfs['Seattle'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like [this](https://plot.ly/python/map-subplots-and-small-multiples/) is the right reference for how to make multiple subplots with maps in plotly.  That example, however, does not use hovertext, and furthermore, each subplot only has one trace.  In the previous example from this notebook, where I made one map for Seattle, I grouped the ports of lading into separate traces, and had the city name and ships sent as the mouse-over text.  It'll require [some modification](https://community.plot.ly/t/adding-traces-to-subplots/4424) to the hyperlinked example code to get this to work.  As a fall-back, maybe I can abandon having multiple traces of groupings of ports of lading, and just use log scaling to make all markers visible on each subplot (since I wouldn't then be able to turn each group off individually).\n",
    "\n",
    "Ok, after playing around with the simpler, `fig.append_trace()` functionality, it's returning errors that \"`'xaxis' is not allowed in 'scattergeo'`\".  It looks like the normal way to do this is to specify a unique '`geo`' arg for each dict in `data`, then add a dict with that '`geo`' label as a keyword within the dict `layout`, and the value for that 'geo' keyword will itself be a dict, which will contain the keyword 'domain', the value of which will be another dict, which will have two keys, 'x' and 'y', each of which will have a list/array with two elements, which will be the limits along the entire figure's x and y axis, as to the outer bounds for that subplot.  [This example](https://plot.ly/python/mixed-subplots/) may be more fruitful a source of inspiration.\n",
    "\n",
    "So, the syntax appears to be:\n",
    "    \n",
    "    trace3 = {\n",
    "        \"geo\": \"geo3\", \n",
    "        \"lon\": df['Longitude'],\n",
    "        \"lat\": df['Latitude'],\n",
    "        \"hoverinfo\": 'text',\n",
    "        \"marker\": {\n",
    "            \"size\": 4,\n",
    "            \"opacity\": 0.8,\n",
    "            \"color\": '#CF1020',\n",
    "            \"colorscale\": 'Viridis'\n",
    "        }, \n",
    "        \"mode\": \"markers\", \n",
    "        \"type\": \"scattergeo\"\n",
    "    }\n",
    "    trace2 = {\n",
    "        \"geo\": \"geo4\"\n",
    "        ...\n",
    "    }\n",
    "    \n",
    "    data = [{trace1}, {trace2}]\n",
    "    \n",
    "    layout = {\n",
    "        ...\n",
    "        \"geo3\": {\n",
    "            \"domain\": {\n",
    "                \"x\": [0, 0.55], \n",
    "                \"y\": [0, 0.9]\n",
    "        }, \n",
    "        \n",
    "    }\n",
    "    \n",
    "It appears that, to get multiple traces in one subplot, you just use the same 'geo' keyword for each, dict appended to data; the layout will then group them all together.  So that's the reason for the somewhat more convoluted way the first example cited above (the one that outputs multiple US maps with WalMart stores over the years): plotly doesn't let you pass a unique axis object associated with each `data` dict when the object is a map, it instead requires that you specify what part of the [0, 1] space of the entire figure's x and y axes you want each trace to occupy, and you indicate that in the layout dict, not within the dict sent to the `data` list for each trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tools\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def plotly_subplots(dict_of_dfs, title, q=4, scale=1, rows=3, cols=2):\n",
    "    tools.set_credentials_file(username=plotly_username, \n",
    "                               api_key=plotly_api_key)\n",
    "\n",
    "    q = 4\n",
    "    colors = [\"rgb(0,116,217)\",\"rgb(255,65,54)\",\"rgb(133,20,75)\",\n",
    "              \"rgb(255,133,27)\",\"lightgrey\"]\n",
    "    data = []\n",
    "    scale = scale\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        font=dict(family='Arial, sans serif',\n",
    "                  size=22,\n",
    "                  color='rgb(0, 0, 0)'\n",
    "                ),\n",
    "        # showlegend = False,\n",
    "        autosize = True,\n",
    "        width = 1000,\n",
    "        height = 900,\n",
    "        hovermode = True,\n",
    "        legend = dict(\n",
    "            x=1,\n",
    "            y=0.5,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0)\",\n",
    "            font = dict( size=11 ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Want one subplot per port of unlading, but multiple groups\n",
    "    # of ports of lading per subplot\n",
    "    geo_s = []\n",
    "    # Iterate over dict of dfs\n",
    "    for i, (city, df) in enumerate(dict_of_dfs.items()):\n",
    "        # Split each df up into quantiles of ports of lading\n",
    "        # Need a special case for Seattle, to keep # groups right\n",
    "        if city == 'Seattle':\n",
    "            q = 5\n",
    "        else:\n",
    "            q = 4\n",
    "        df['quant'] = pd.qcut(\n",
    "            x=df['containers'], q=q, precision=0, duplicates='drop')\n",
    "        # Specify geo_key, which will tell \n",
    "        # layout_dict where to place subplot\n",
    "        geo_key = 'geo'+str(i+1) if i != 0 else 'geo'\n",
    "        # Go through groups of cities and plot markers\n",
    "        for j, (interval, group) in enumerate(df.groupby('quant')):\n",
    "            lim = interval\n",
    "            label = '{}: <br />{} - {} containers'.format(\n",
    "                city.replace('_', ' '), \n",
    "                np.rint(lim.left).astype(int),\n",
    "                np.rint(lim.right).astype(int)-1)\n",
    "            group_data = dict(\n",
    "                type = 'scattergeo',\n",
    "                geo = geo_key,\n",
    "                locationmode = 'ISO-3',\n",
    "                lat = group['lat'],\n",
    "                lon = group['long'],\n",
    "                text = group['text'],\n",
    "                marker = dict(\n",
    "                    size = group['containers']/scale,\n",
    "    #                 size = np.log(group['containers'])*50,\n",
    "                    color = colors[j],\n",
    "                    line = dict(width=0.5, color='rgb(40,40,40)'),\n",
    "                    sizemode = 'area'\n",
    "                ),\n",
    "                name = label,\n",
    "                legendgroup = label\n",
    "            )\n",
    "            data.append(group_data)\n",
    "\n",
    "        # Add a clear map to show plot titles\n",
    "        data.append(\n",
    "            dict(\n",
    "                type = 'scattergeo',\n",
    "                showlegend = False,\n",
    "                # Always put the text in the same place;\n",
    "                #\n",
    "                lon = [-115],\n",
    "                lat = [-60],\n",
    "                geo = geo_key,\n",
    "                text = '{}'.format(city.replace('_', ' ')),\n",
    "                mode = 'text',\n",
    "                textfont=dict(\n",
    "                    family='Arial, sans serif',\n",
    "                    size=22,\n",
    "                    color='rgb(0, 0, 0)'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layout[geo_key] = dict(\n",
    "            scope='world',\n",
    "            projection=dict( type='Mercator' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "            subunitwidth=1,\n",
    "            countrywidth=1,\n",
    "            subunitcolor=\"rgb(255, 255, 255)\",\n",
    "            countrycolor=\"rgb(255, 255, 255)\",\n",
    "            domain = dict( x = [], y = [] ),\n",
    "        )\n",
    "\n",
    "\n",
    "    z = 0\n",
    "    COLS = cols\n",
    "    ROWS = rows\n",
    "    for y in reversed(range(ROWS)):\n",
    "        for x in range(COLS):\n",
    "            geo_key = 'geo'+str(z+1) if z != 0 else 'geo'\n",
    "            layout[geo_key]['domain']['x'] = [float(x)/float(COLS), \n",
    "                                              float(x+1)/float(COLS)]\n",
    "            layout[geo_key]['domain']['y'] = [float(y)/float(ROWS), \n",
    "                                              float(y+1)/float(ROWS)]\n",
    "            z=z+1\n",
    "            if z > len(data):\n",
    "                break\n",
    "    subp_data = data\n",
    "    subp_layout = layout\n",
    "    \n",
    "    return data, layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/32.embed\" height=\"900px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '<b>Ports of Lading Presented by <br />\\\n",
    "Ports of Unlading: 2017</b>'\n",
    "\n",
    "data, layout = plotly_subplots(city_counts_dfs, title=title)\n",
    "\n",
    "fig = { 'data':data, 'layout':layout }\n",
    "py.iplot( fig, filename='Ports of Lading', height=900, width=1000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My First Plotly Maps in Subplots Figure\n",
    "\n",
    "Ok, after a lot of fiddling, I finally got that to look pretty much the way that I want.  With such a specific plotting agenda, it's probably difficult to avoid a lot of fiddling to get the parameters right.\n",
    "\n",
    "Another word on the learning curve: plotly is based on D3.js, i.e. Javascript, and thus has a different kind of feel than the typical Python library, in my experience.  In particular, it seems to involve a lot more dicts, and nested dicts.  The easy access to maps is pretty cool, but for some reason it makes it incompatible with easy creation/access to subplots, thus the comment in the cell above about having to create text annotation via a bunch of independent traces mapped to the same figure-grid coordinates.  Walking through [the example code](https://plot.ly/python/map-subplots-and-small-multiples/) about Walmart stores was very helpful in getting the plot to eventually render for me.  Basically, you feed everything in to one data object (a list of dicts) and one layout object (a dict), and link the two with the `geo` key arg.\n",
    "\n",
    "---\n",
    "\n",
    "So, finally: time to consider some implications of the data.  Yes, Seattle is unique in receiving so much traffic from Vancouver.  Tacoma has far, far less, and in general processes more East Asian traffic, and less from Latin America or Europe.  LA and LBC process the most cargo, overall, and most of it from China: Hong Kong (Yantian) is tops for LA, and Shanghai sends the most to Long Beach.  Both also get a considerable amount from Latin America and Europe; kind of like Seattle's profile, in that regard.\n",
    "\n",
    "**One significant point that I just realized: I've been saying (or at least implying) that individual rows in the data represent unique ships, but now see that they're individual containers, many of them on the same ship.  It's an example of not looking over your data enough in preliminary analysis before jumping into visualization and in-depth analysis.  So I've changed the legend above and will try to fix the rest of the notebook; if I overlook any prior to this point, please disregard.  Such a major oversight should have been fairly obvious, but I'm not really that familiar with the subject matter: 27 million containers a year is a lot more believable than 27 million ships in a year, but either one is still kind of hard to imagine.**\n",
    "\n",
    "The two major ports that aren't on the West Coast are interesting: Houston gets a lot more traffic from the Gulf side of Mexico, and from the Atlantic coast of South America, as well as being a pretty major import hub from Europe.  Newark, however, is of course *the* entrepot of Atlantic traffic, although they do also get a lot from South Asia and even East Asia.\n",
    "\n",
    "Some anomalies are interesting, however.  NYC receives a lot of cargo from a place called South Riding Point, Bahamas.  2,166 containers in 2017, to be exact.  There's no way that the Bahamas are a major exporter of just about anything on that scale, so this is probably more evidence of some kind of economic shenanigans wherein the place where goods are loaded for export isn't necessarily the same country where they're produced.  Other points: Africa and the Middle East aren't huge hotspots for exporting to the major US hubs, and, perhaps more surprisingly, neither is Australia.  This could be evidence that extremes of development in nations decrease the likelihood that it will be a big exporter to the US, but the amount of goods we get from Europe would seem to weaken such a conclusion.\n",
    "\n",
    "Then again, this is just one look at some of the biggest ports in the US, and visual analysis with a map, while intuitive and immediate, is less thorough, and the largest markers easily dominate the visual field when doing a quick survey.  The 6 cities shown above account for just over 400,000 rows of shipping records from 2017, out of a total 27 million rows in the entire dataset.  It would be interesting to create a heatmap of all lat-long origins of shipping bound to the US, either from the entire dataset (if my laptop can handle it), or just from these cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Lat-Lon for Ports of Lading\n",
    "\n",
    "First, though, let's just try something extremely simple: just find the 'center of mass' for each port of unlading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Long Beach, California': (33.7700504, -118.1937395), 'Los Angeles, California': (34.0522342, -118.2436849), 'Tacoma, Washington': (47.2528768, -122.4442906), 'Seattle, Washington': (47.6062095, -122.3320708), 'Houston, Texas': (29.7604267, -95.3698028), 'Newark, New Jersey': (40.735657, -74.1723667)} \n",
      "\n",
      " {'Houston': 'Houston, Texas', 'Long_Beach': 'Long Beach, California', 'Los_Angeles': 'Los Angeles, California', 'NYC_Newark': 'Newark, New Jersey', 'Seattle': 'Seattle, Washington', 'Tacoma': 'Tacoma, Washington'}\n"
     ]
    }
   ],
   "source": [
    "# # Note: Already run; load from paths['top_6_unlading_pkl']\n",
    "\n",
    "# from geopy import geocoders\n",
    "\n",
    "# unlading_ports = ['Long Beach, California', 'Los Angeles, California', 'Tacoma, Washington',\n",
    "#                   'Seattle, Washington', 'Houston, Texas', 'Newark, New Jersey']\n",
    "# unlading_coords = dict()\n",
    "# failures = []\n",
    "# geolocator = geocoders.GoogleV3(api_key=google_api_key,\n",
    "#                                     timeout=100)\n",
    "# for port in unlading_ports:\n",
    "#         location = geolocator.geocode(port)\n",
    "#         if location:\n",
    "#             lat = location.latitude\n",
    "#             long = location.longitude\n",
    "#             unlading_coords[port] = (lat, long)\n",
    "#         else:\n",
    "#             failures.append(port)\n",
    "\n",
    "with open(paths['top_6_unlading_pkl'], 'rb') as f:\n",
    "    unlading_coords = pickle.load(f)\n",
    "    \n",
    "name_translator = dict()\n",
    "for (key, city) in zip(sorted(unlading_coords.keys()), sorted(cities_2017_dfs.keys())):\n",
    "    name_translator[city] = key\n",
    "print(unlading_coords, '\\n\\n', name_translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tools\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def plotly_avg_coords(dict_of_dfs, title, scale=1E3, width=1000, height=600):\n",
    "    tools.set_credentials_file(username=plotly_username, \n",
    "                               api_key=plotly_api_key)\n",
    "\n",
    "    colors = [\"rgb(0,116,217)\",\"rgb(255,65,54)\",\"rgb(133,20,75)\",\n",
    "              \"rgb(255,133,27)\",\"rgb(40,40,40)\",'rgb(100,200,50)']\n",
    "    data = []\n",
    "    scale = scale\n",
    "    log_scale = 3\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        font=dict(family='Arial, sans serif',\n",
    "                  size=18,\n",
    "                  color='rgb(0, 0, 0)'\n",
    "                ),\n",
    "        showlegend = True,\n",
    "        autosize = False,\n",
    "        width = width,\n",
    "        height = height,\n",
    "        hovermode = True,\n",
    "        legend = dict(\n",
    "            x=1,\n",
    "            y=0.5,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0)\",\n",
    "            font = dict( size=11 ),\n",
    "        ),\n",
    "        geo = dict(\n",
    "                scope='world',\n",
    "                projection=dict( type='Mercator' ),\n",
    "                showland = True,\n",
    "                landcolor = 'rgb(217, 217, 217)',\n",
    "                subunitwidth=1,\n",
    "                countrywidth=1,\n",
    "                subunitcolor=\"rgb(255, 255, 255)\",\n",
    "                countrycolor=\"rgb(255, 255, 255)\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Iterate over dict of dfs\n",
    "    for i, (city, df) in enumerate(dict_of_dfs.items()):\n",
    "        city_read = '{}'.format(city.replace('_', ' '))\n",
    "        total_weights = df['containers'].sum()\n",
    "        lat_weight = df['lat'] * df['containers']\n",
    "        lat_avg = lat_weight.sum()/total_weights\n",
    "        lon_weight = df['long'] * df['containers']\n",
    "        lon_avg = lon_weight.sum()/total_weights\n",
    "        un_lat = unlading_coords[name_translator[city]][0]\n",
    "        un_lon = unlading_coords[name_translator[city]][1]\n",
    "        geo_key = 'geo'\n",
    "\n",
    "        # Go through cities and plot markers to 'center of mass'    \n",
    "        center_of_mass = dict(\n",
    "            type = 'scattergeo',\n",
    "            geo = geo_key,\n",
    "            locationmode = 'world',\n",
    "            lat = [lat_avg],\n",
    "            lon = [lon_avg],\n",
    "            text = '\"Center of mass\" of ports <br />\\\n",
    "    exporting to {}'.format(city_read),\n",
    "            showlegend = False,\n",
    "            marker = dict(\n",
    "                size = df['containers'].sum()/scale,\n",
    "    #             size = np.log(df['containers'].sum())*log_scale,\n",
    "                color = colors[i],\n",
    "                line = dict(width=0.5, color='rgb(40,40,40)'),\n",
    "                sizemode = 'area',\n",
    "                opacity = 1\n",
    "            ),\n",
    "            name = '{}'.format(city_read),\n",
    "            legendgroup = '{}'.format(city_read)\n",
    "        )\n",
    "        data.append(center_of_mass)\n",
    "\n",
    "        # Add a clear map to show port of unlading\n",
    "        un_port = dict(\n",
    "            type = 'scattergeo',\n",
    "            geo = geo_key,\n",
    "            locationmode = 'world',\n",
    "            lat = [un_lat],\n",
    "            lon = [un_lon],\n",
    "            text = '{}'.format(city_read),\n",
    "            marker = dict(\n",
    "                size = 10,\n",
    "                color = colors[i],\n",
    "                line = dict(width=0.5, color='rgb(40,40,40)'),\n",
    "                sizemode = 'area'\n",
    "            ),\n",
    "            name = '{}'.format(city_read),\n",
    "            showlegend = False\n",
    "#             legendgroup = '{}'.format(city_read)\n",
    "        )\n",
    "        data.append(un_port)\n",
    "\n",
    "        # One more: add lines connecting ports of unlading\n",
    "        # with their respective centers of mass\n",
    "        line = dict(\n",
    "            geo = 'geo',\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'world',\n",
    "            lon = [un_lon, lon_avg],\n",
    "            lat = [un_lat, lat_avg],\n",
    "            mode = 'lines',\n",
    "            name = city_read,\n",
    "            showlegend = True,\n",
    "            legendgroup = '{}'.format(city_read),\n",
    "            line = dict(\n",
    "                width = 1,\n",
    "                color = colors[i],\n",
    "            )\n",
    "        )\n",
    "        data.append(line)\n",
    "        \n",
    "    return data, layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/38.embed\" height=\"600px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '<b>Average Lat-Lon of Ports of Lading <br />\\\n",
    "Presented by Ports of Unlading: 2017</b>'\n",
    "\n",
    "data, layout = plotly_avg_coords(city_counts_dfs, title)\n",
    "        \n",
    "fig = { 'data':data, 'layout':layout }\n",
    "py.iplot( fig, filename='CofM Ports of Lading', height=600, width=1200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that took longer than I intended to format a new map, but it looks like a useful summary of the places from which different cities receive imports.  Tacoma, Long Beach, and LA all pretty solidly receive East Asian imports, overwhelming everything else.  But Seattle and Houston have as much stuff coming from East and West that the average longitude of a container coming in is pretty close to zero (Greenwich).  Just to see how stable this pattern is, let's compare to the earliest year for which Enigma has these data: 2014.\n",
    "\n",
    "When I went back to the relevant page, the difference is striking; while there are far fewer records for that year (the whole db is 11 million, as opposed to the ~28 million rows for the 2017 database), the top 6 ports of unlading account for a much larger proportion of overall traffic:\n",
    "\n",
    "    Frequency Rank for Port of Unlading in 2014:\n",
    "    Los Angeles -  2,025,070\n",
    "    Long Beach  -  1,670,075\n",
    "    NYC/Newark  -  1,178,130\n",
    "    Tacoma      -    908,218\n",
    "    Houston     -    676,386\n",
    "    Total rows  - 11,025,607\n",
    "    \n",
    "That means that Los Angeles handled 5% of the cargo in 2017 compared to what it received in 2014?  That doesn't seem right.  [This page](https://www.portoflosangeles.org/maritime/stats.asp) of annual traffic to the Port of Los Angeles contradicts Enigma's data, showing a gradual but consistent *increase* in containers (twenty-foot-equivalent units) handled by that port over the last decade, and indeed going back to the 80s.  Their monthly breakdown shows over 4 million inbound loaded TEUs for the last few years.  [Long Beach's site](http://www.polb.com/economics/stats/yearly_teus.asp) similarly lists pretty consistent volumes, between 3 and 4 million every year.  Obviously Enigma's data, while extensive, aren't comprehensive or authoritative.  Let's check the rest of the years for Enigma's data sets:\n",
    "\n",
    "    Frequency Rank for Port of Unlading in 2015:\n",
    "    Los Angeles -    291,567\n",
    "    Long Beach  -    237,690\n",
    "    NYC/Newark  -    211,192\n",
    "    Seattle     -    149,245\n",
    "    Tacoma      -    149,245\n",
    "    Total rows  -  1,914,262\n",
    "    \n",
    "    Frequency Rank for Port of Unlading in 2016:\n",
    "    Los Angeles -  5,024,788\n",
    "    NYC/Newark  -  4,468,536\n",
    "    Long Beach  -  3,979,732\n",
    "    Tacoma      -  2,272,418\n",
    "    Savannah    -  1,827,115\n",
    "    Total rows  - 29,209,522\n",
    "    \n",
    "Obviously, the datasets are highly variable.  The top ports of unlading across the years are pretty consistent, but the total data present are highly variable, and the number listed for each US port similarly fluctuate.  For a less biased look, [the Census Bureau](https://www.census.gov/foreign-trade/index.html) claims to be the official source of statistics on imports and exports in the US.  The [International Trade Administration](https://www.trade.gov/mas/ian/tradestatistics/index.asp) also keeps some data.  Their [latest summary](https://www.trade.gov/mas/ian/build/groups/public/@tg_ian/documents/webcontent/tg_ian_005537.pdf), for 2016, lists total imports for the US at 2.7 trillion USD that year.\n",
    "\n",
    "BTW, while looking into getting more complete access to summary statistics for Enigma's datasets, I came across [this page](https://docs.enigma.com/public/public_v20_user_python.html), which looks like the most accessible way to get programmatic access to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll run a brief trial of the [programmatic access](https://docs.enigma.com/public/public_v20_user_python.html) below, since the 2014 data set was too large to actually download.  I'd like to see whether there's a way to query the db without actually loading it into local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "enigma_api = 'Bearer {}'.format(credentials['enigma']['api_key'])\n",
    "headers = {'authorization': enigma_api}\n",
    "base_url = \"https://public.enigma.com/api/\"\n",
    "\n",
    "def print_top_level_collections():\n",
    "    url = base_url + \"collections/\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    collections = r.json()\n",
    "    for collection in collections:\n",
    "        print(collection['display_name'])\n",
    "        print(collection['description_short'])\n",
    "        print(collection['id'] + \"\\n\")\n",
    "        \n",
    "def find_current_snapshot_id(dataset_id):\n",
    "    url = base_url + \"datasets/\" + dataset_id\n",
    "    r = requests.get(url, headers=headers)\n",
    "    dataset = r.json()\n",
    "    return dataset['current_snapshot']['id']\n",
    "\n",
    "def snapshots_in_collection(collection_id):\n",
    "    url = base_url + \"datasets/?parent_collection_id=\" + collection_id\n",
    "    r = requests.get(url, headers=headers)\n",
    "    datasets = r.json()\n",
    "    return [\n",
    "        dataset['current_snapshot']['id']\n",
    "        for dataset in datasets\n",
    "            if dataset['current_snapshot'] is not None\n",
    "        ]\n",
    "\n",
    "def get_matching_rows(snapshot_id, search_string):\n",
    "    url = base_url + \"snapshots/\" + snapshot_id + \"?row_limit=1000&query=\" + search_string\n",
    "    r = requests.get(url, headers=headers)\n",
    "    snapshot = r.json()\n",
    "    return snapshot['table_rows']['rows']\n",
    "\n",
    "def get_matching_rows(snapshot_id, search_string, field_index):\n",
    "    url = base_url + \"snapshots/\" + snapshot_id + \"?row_limit=1000&query=\" + search_string\n",
    "    r = requests.get(url, headers=headers)\n",
    "    snapshot = r.json()\n",
    "    rows = snapshot['table_rows']['rows']\n",
    "    return [\n",
    "        row\n",
    "        for row in rows if search_string in row[field_index].lower()\n",
    "        ]\n",
    "\n",
    "def get_dataset_ids_in_collection(collection_id, max):\n",
    "    url = base_url + \"datasets/?parent_collection_id=\" + collection_id\n",
    "    headers['Range'] = 'resources=%d-%d' % (0, max - 1)\n",
    "    r = requests.get(url, headers = headers)\n",
    "    datasets = r.json()\n",
    "    return [\n",
    "        dataset['id']\n",
    "        for dataset in datasets\n",
    "        ]\n",
    "\n",
    "def get_all_datasets_in_collection(collection_id):\n",
    "    url = base_url + \"datasets/?parent_collection_id=\" + collection_id\n",
    "    r = requests.head(url, headers = headers)\n",
    "    num_datasets = int(r.headers.get('content-range').split(\"/\")[1])\n",
    "    dataset_ids = []\n",
    "    for start in range(0, num_datasets, 10):\n",
    "        headers['Range'] = 'resources=%d-%d' % (start, start + 9)\n",
    "        r = requests.get(url, headers=headers)\n",
    "        datasets = r.json()\n",
    "        for dataset in datasets:\n",
    "            dataset_ids.append(dataset['id'])\n",
    "    return dataset_ids\n",
    "        \n",
    "# print_top_level_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_of_lading_summary_id = '057fee6f-ec01-47c5-990d-02c1e35aaedc'\n",
    "bills_of_lading_2014_id = '69806aa4-3090-4b2c-878c-8231f3791f17'\n",
    "# snapshots_in_collection(bills_of_lading_2014_id)\n",
    "\n",
    "get_all_datasets_in_collection(bill_of_lading_summary_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "dataset_id = '69806aa4-3090-4b2c-878c-8231f3791f17'\n",
    "snapshot_id = find_current_snapshot_id(dataset_id)\n",
    "search_string = 'Seattle'\n",
    "cols = ['foreign_port_of_lading']\n",
    "url = base_url + \"snapshots/\" + snapshot_id + \"?row_limit=10000&query=\" + search_string\n",
    "# url = base_url + \"datasets/?parent_collection_id=\" + dataset_id + \"?row_limit=1000000&query=\" + search_string\n",
    "response = requests.get(url, headers=headers).content\n",
    "# seattle_shipping_2014 = pd.read_csv(io.StringIO(response.decode('utf-8')), usecols=cols)\n",
    "# df.head()\n",
    "# response[:100]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above contains the indicated structure for retrieving rows from the 2014 Bills of Lading collection, but if I try substituting \"1000000\" into the 'row_limit' string, it returns the response that their exploratory mode is limited to 10000 (ten thousand) records at a time.  Therefore there doesn't appear to be a programmatic way to retrieve a large subset of the data set using a limiting criterion.  I'll instead return to just downloading csv files and loading them.  Given the huge sizes involved, however, I'll stick to just comparing imports to Seattle over the years available.\n",
    "\n",
    "## Seattle Imports Across the Years\n",
    "\n",
    "The respective number of rows matching Seattle as the port of unlading for each year are:\n",
    "\n",
    "    2014 - 492,215\n",
    "    2015 - 149,267\n",
    "    2016 - 923,437\n",
    "    2017 -  27,100\n",
    "    \n",
    "Their csvs, once downloaded, occupy [302, 95, 523, 15] MBs, respectively, on the local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seattle_csvs = []\n",
    "for i in range(4, 8):\n",
    "    seattle_csvs.append('seattle_shipping_201{}_csv'.format(i))\n",
    "    \n",
    "seattle_lading_years = {}\n",
    "for file in seattle_csvs:\n",
    "    year = file.split(sep='_')[2]\n",
    "    seattle_lading_years[year] = pd.read_csv(paths[file], \n",
    "                                             usecols=['foreign_port_of_lading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai ,China (Mainland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tanjung Pelepas,Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shanghai ,China (Mainland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shanghai ,China (Mainland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pusan,South Korea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       foreign_port_of_lading\n",
       "0  Shanghai ,China (Mainland)\n",
       "1    Tanjung Pelepas,Malaysia\n",
       "2  Shanghai ,China (Mainland)\n",
       "3  Shanghai ,China (Mainland)\n",
       "4           Pusan,South Korea"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle_lading_years['2014'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some preliminary cleaning, before I can grab coords for each city.  Need to drop any NaN, and identify any 5-digit codes in place of city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foreign_port_of_lading    0\n",
      "dtype: int64\n",
      "foreign_port_of_lading    0\n",
      "dtype: int64\n",
      "foreign_port_of_lading    0\n",
      "dtype: int64\n",
      "foreign_port_of_lading    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for year, series in seattle_lading_years.items():\n",
    "    print(series.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_lading_years['2014'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014:\n",
      "Series([], Name: foreign_port_of_lading, dtype: int64)\n",
      "2015:\n",
      "57045    2026\n",
      "57073     101\n",
      "35136       2\n",
      "57019       1\n",
      "Name: foreign_port_of_lading, dtype: int64\n",
      "2016:\n",
      "57073    2214\n",
      "57045     173\n",
      "53304      38\n",
      "35136      29\n",
      "99999       5\n",
      "35180       5\n",
      "57037       3\n",
      "55225       1\n",
      "Name: foreign_port_of_lading, dtype: int64\n",
      "2017:\n",
      "57037    721\n",
      "57073      2\n",
      "35136      1\n",
      "Name: foreign_port_of_lading, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "code_matches = code_lookup(seattle_lading_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'99999', '53304', '57037', '35136', '55225', '57019', '57045', '35180', '57073'}\n"
     ]
    }
   ],
   "source": [
    "print(code_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999\n",
      "35136\n",
      "55225\n",
      "57019\n",
      "57045\n",
      "35180\n"
     ]
    }
   ],
   "source": [
    "origin_codes_dict = {'57073': 'Nansha, China (Mainland)', \n",
    "                     '53304': 'Pipavav, India', \n",
    "                     '30110': 'Covenas, Columbia', \n",
    "                     '57037': 'Yangshan, China (Mainland)'\n",
    "                    }\n",
    "for code in code_matches:\n",
    "    if code not in origin_codes_dict.keys():\n",
    "        print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That yielded just one new hit, when comparing to [these](https://www.cbp.gov/sites/default/files/assets/documents/2017-Feb/appendix_f_0.pdf) [two](http://customscodes.com/foreign_port_code.php) resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_codes_dict[57019] = 'Zhenjiang, China'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try both of the graph types above: I'll start with the averaged lat/lon approach.  First I need to clean up the port names and retrieve lat/lon for them, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014        \n",
      "Coordinates couldn't be found for  1317 ports of lading; 0.27% of all records\n",
      "2015        \n",
      "Coordinates couldn't be found for   903 ports of lading; 0.61% of all records\n",
      "2016        \n",
      "Coordinates couldn't be found for  1615 ports of lading; 0.17% of all records\n",
      "2017        \n",
      "Coordinates couldn't be found for     1 ports of lading; 0.00% of all records\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cities_2017_dfs = import_shipping_data(cities_2017_paths)\n",
    "# seattle_lading_coded = replace_city_codes(seattle_lading_years, origin_codes_dict)\n",
    "# seattle_lading_formatted = format_ports(seattle_lading_coded)\n",
    "\n",
    "# lat_long, failures = geolocate(seattle_lading_formatted)\n",
    "\n",
    "# seattle_lading_coords = get_coords(seattle_lading_formatted, lat_long)\n",
    "# seattle_lading_counts = get_counts(seattle_lading_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>containers</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>(22.396428, 114.109497)</td>\n",
       "      <td>22.396428</td>\n",
       "      <td>114.109497</td>\n",
       "      <td>940</td>\n",
       "      <td>Hong Kong, Hong Kong: 940 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tsingtao, China</td>\n",
       "      <td>(36.067108, 120.382609)</td>\n",
       "      <td>36.067108</td>\n",
       "      <td>120.382609</td>\n",
       "      <td>23</td>\n",
       "      <td>Tsingtao, China: 23 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>(35.1795543, 129.0756416)</td>\n",
       "      <td>35.179554</td>\n",
       "      <td>129.075642</td>\n",
       "      <td>3240</td>\n",
       "      <td>Pusan, South Korea: 3240 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yantian, China</td>\n",
       "      <td>(22.556499, 114.236875)</td>\n",
       "      <td>22.556499</td>\n",
       "      <td>114.236875</td>\n",
       "      <td>999</td>\n",
       "      <td>Yantian, China: 999 container(s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shanghai, China</td>\n",
       "      <td>(31.2303904, 121.4737021)</td>\n",
       "      <td>31.230390</td>\n",
       "      <td>121.473702</td>\n",
       "      <td>2936</td>\n",
       "      <td>Shanghai, China: 2936 container(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    city                   lat_long        lat        long  \\\n",
       "0   Hong Kong, Hong Kong    (22.396428, 114.109497)  22.396428  114.109497   \n",
       "3        Tsingtao, China    (36.067108, 120.382609)  36.067108  120.382609   \n",
       "4     Pusan, South Korea  (35.1795543, 129.0756416)  35.179554  129.075642   \n",
       "5         Yantian, China    (22.556499, 114.236875)  22.556499  114.236875   \n",
       "11       Shanghai, China  (31.2303904, 121.4737021)  31.230390  121.473702   \n",
       "\n",
       "    containers                                    text  \n",
       "0          940  Hong Kong, Hong Kong: 940 container(s)  \n",
       "3           23        Tsingtao, China: 23 container(s)  \n",
       "4         3240   Pusan, South Korea: 3240 container(s)  \n",
       "5          999        Yantian, China: 999 container(s)  \n",
       "11        2936      Shanghai, China: 2936 container(s)  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle_lading_counts['2017'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Port Redon, Vietnam', 'High Seas, United States Off Shore Tanker Transshipment Points', 'Pursan, Turkey', 'Nagoya Ko, Japan', 'Puduchcheri, India', 'All Other Southern Asia, N E C Ports, Southern Asia, N E C', 'All Other Oman Ports, Oman']\n"
     ]
    }
   ],
   "source": [
    "print(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that looks good.  I'll save the counts, and the dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293 245\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "with open(paths['lat_long_dict_pkl'], 'rb') as f:\n",
    "    old_dict = pickle.load(f)\n",
    "    \n",
    "print(len(old_dict.keys()), len(lat_long.keys()))\n",
    "new_dict = old_dict\n",
    "for city, coords in lat_long.items():\n",
    "    if city not in old_dict.keys():\n",
    "        new_dict[city] = coords\n",
    "        \n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['seattle_annual_pkl'] = os.path.join(paths['data_dir'], 'seattle_annual.pkl')\n",
    "\n",
    "with open(paths['lat_long_dict_pkl'], 'wb') as f:\n",
    "    pickle.dump(new_dict, f)\n",
    "with open(paths['seattle_annual_pkl'], 'wb') as f:\n",
    "    pickle.dump(seattle_lading_counts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paths['seattle_annual_pkl'], 'rb') as f:\n",
    "    seattle_lading_counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to check the plots for multiple years of Seattle bills of lading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in seattle_lading_counts.keys():\n",
    "    name_translator[year] = 'Seattle, Washington'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/38.embed\" height=\"600px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '<b>Average Lat-Lon of Ports of Lading <br />\\\n",
    "Bound for Seattle: 2014-2017</b>'\n",
    "\n",
    "data, layout = plotly_avg_coords(seattle_lading_counts, title, scale=5E3)\n",
    "        \n",
    "fig = { 'data':data, 'layout':layout }\n",
    "py.iplot( fig, filename='CofM Ports of Lading', height=600, width=1200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so, for Seattle at least, the average number of containers coming from ports didn't shift very far East or West for the years 2014-2016.  The data are highly variable for those years; many more records from 2016 than 2014 (and note that I had to change the scale to make the markers more visible, compared to the 2017 data mapped earlier).  2017 has far fewer records for shipments to Seattle, and therefore it's less surprising that the geographic center of their distribution also shifted.  This almost certainly represents bias in sampling in the data, rather than an actual shift in the number and origin points of cargo received; as indicated above, Enigma's data are highly variable for other ports, and in ways that don't line up with the largest ports' own records.\n",
    "\n",
    "I found [some historical data](https://www.portseattle.org/About/Publications/Statistics/Seaport/Pages/10-Year-History.aspx) for the Port of Seattle, and there is indeed significant variety over the years indicated, but container imports between 500K and 1 million are actually believable.  For some reason the data on that page don't cover more recent years; the [most recent complete report](https://www.nwseaportalliance.com/sites/default/files/nwsa_annualreport_2016_full_0.pdf) I could find lists 3.6 million total TEUs, but doesn't break them down by import full/empty vs export full/empty.\n",
    "\n",
    "For the hell of it, let's look at the city-by-city breakdown of ports of lading for Seattle by year, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mackellardrew/32.embed\" height=\"900px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '<b>Ports of Lading for Containers Received in Seattle, <br />\\\n",
    "Presented by Year: 2014-2017</b>'\n",
    "\n",
    "data, layout = plotly_subplots(seattle_lading_counts, scale=50, title=title, rows=2)\n",
    "\n",
    "fig = { 'data':data, 'layout':layout }\n",
    "py.iplot( fig, filename='Ports of Lading', height=900, width=1000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too surprising; the breakdown looks pretty much the same for most years; mostly East Asia, but also good representation of the E.U. and a smattering of Latin America.  Importantly, the phenomenon of a ton of cargo coming via British Columbia has existed for all of those years.\n",
    "\n",
    "I'd say that's enough for enumerating ports of lading for containers.  Let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_lading_counts = {}\n",
    "for year, df in seattle_lading_coords.items():\n",
    "    seattle_lading_counts[year] = df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_lading_coords['2017'].groupby(['foreign_port_of_lading']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, layout = plotly_avg_coords(city_counts_dfs, title)\n",
    "        \n",
    "fig = { 'data':data, 'layout':layout }\n",
    "py.iplot( fig, filename='CofM Ports of Lading', height=600, width=1200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, at the risk of spending too much time learning another mapping paradigm in plotly, let's see if I can create such a summary.  I'll just use the data on hand, rather than all 27 million records for 2017, at this point.  I don't know if it'll be easy to overlay a heatmap onto an actual geo world map.  One alternative is to do a [choropleth map](https://plot.ly/python/choropleth-maps/), but I'd want really small units demarcating areas, so the resolution of the shipping data could be displayed effectively.  If I want to try that, there's [precedent](https://community.plot.ly/t/create-your-own-choropleth-map-with-custom-shapefiles/2567/4) for how to feed in custom boundaries, and a data source for getting higher resolution areas than just national borders is [here](http://devecondata.blogspot.com/2007/09/administrative-boundaries.html).  I'll download the multi-level shapefile to the 'data_dir' path.\n",
    "\n",
    "As [this discussion](https://community.plot.ly/t/what-is-the-best-way-to-put-a-heatmap-on-a-map/3661/2) points out, another approach might be to use explicit markers like above, but make their opacity dependent upon the number of overlapping points represented.  An example that they cite, which looks pretty good, is [here](https://i.stack.imgur.com/9ek6k.png).  Of course, in my case, the cities' lat/long coordinates are all identical, so you'd end up with tiny dots that just get slightly darker, which wouldn't give the exact impression I'm going for.  You could, however, use a function from either numpy or python's built-in random module to introduce uniformly distributed permutation of the lat-long coords based on how many records come from a given port, in order to spread the effect out spatially.  But I don't think I could contain the effect based on shorelines, meaning it would bleed over into surrounding water.  That might be ok, but it does sound like quite a bit of fiddling to set it up.  One additional idea would be to specify a different color along a range of colors available on a yellow-orange-red spectrum that the markers could assume, based on the total number of containers coming from that city.  But that might wash out the original message of the opacity-based approach.\n",
    "\n",
    "...Ah, let's just try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new, combined df of ports of lading\n",
    "# Keeping only city name, identifier, lat, and long\n",
    "frames = []\n",
    "columns = ('container_number', 'foreign_port_of_lading', 'lat_long')\n",
    "for city, df in cities_2017_dfs.items():\n",
    "    frames.append(df.loc[:, columns])\n",
    "top_6_df = pd.concat(frames)\n",
    "top_6_df.sort_values(by='foreign_port_of_lading', inplace=True)\n",
    "print('top_6_df.shape: {}'.format(top_6_df.shape))\n",
    "\n",
    "# Specify a dict linking coords and total counts\n",
    "all_counts = top_6_df['lat_long'].value_counts().to_dict()\n",
    "print('\\nCounts for first 5 cities in top_6_df:')\n",
    "for k, v in {k: all_counts[k] for k in sorted(all_counts.keys())[:5]}.items():\n",
    "    print('{:<35} {}'.format(str(k), v))\n",
    "top_6_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_6_per = top_6_df\n",
    "max_count = top_6_df['foreign_port_of_lading'].value_counts().max()\n",
    "factor=1\n",
    "\n",
    "# Break out lat/long into separate columns, with noise \n",
    "# based on how highly that city's count ranks\n",
    "top_6_per['lat'] = top_6_per['lat_long'].apply(\n",
    "    lambda x: (x)[0]+np.random.normal(\n",
    "        loc=0, scale=all_counts[x]*factor/max_count))\n",
    "top_6_per['long'] = top_6_per['lat_long'].apply(\n",
    "    lambda x: (x)[1]+np.random.normal(\n",
    "        loc=0, scale=all_counts[x]*factor/max_count))\n",
    "\n",
    "top_6_per.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I tried running this, plotly's server returned a friendly warning that there are too many data points, and that it bogs down with more than 40,000 points in scatter mode.  I have about 10x that.  To get a look at whether or not the formatting makes sense, let's try just running with a subset.  I'll choose cities with the max, min, and median total exports, and see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = top_6_df['foreign_port_of_lading'].value_counts().max()\n",
    "min_count = top_6_df['foreign_port_of_lading'].value_counts().min()\n",
    "med_count = top_6_df['foreign_port_of_lading'].value_counts().median()\n",
    "subset_index = []\n",
    "for k, v in all_counts.items():\n",
    "    if v in [max_count, min_count, med_count]:\n",
    "        subset_index.append(k)\n",
    "        \n",
    "subset = top_6_per.set_index(['lat_long']).loc[subset_index]\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.set_credentials_file(username=plotly_username, api_key=plotly_api_key)\n",
    "\n",
    "top_6_plus = cities_2017_dfs\n",
    "top_6_plus.update(top_6=top_6_df)\n",
    "\n",
    "colors = [\"rgb(0,116,217)\",\"rgb(255,65,54)\",\"rgb(133,20,75)\",\"rgb(255,133,27)\",\"lightgrey\"]\n",
    "cities = []\n",
    "markersize = 3\n",
    "scale = 3\n",
    "log_scale = 100\n",
    "\n",
    "city = dict(\n",
    "    type = 'scattergeo',\n",
    "    locationmode = 'ISO-3',\n",
    "#     lon = top_6_per['long'],\n",
    "#     lat = top_6_per['lat'],\n",
    "#     text = top_6_per['foreign_port_of_lading'],\n",
    "    lon = subset['long'],\n",
    "    lat = subset['lat'],\n",
    "    text = subset['foreign_port_of_lading'],\n",
    "    marker = dict(\n",
    "        size = 5,\n",
    "        color = 'rgb(100, 100, 100, 0.1)',\n",
    "        line = dict(width=0.5, color='rgb(40,40,40)'),\n",
    "        sizemode = 'area'\n",
    "    )\n",
    ")\n",
    "cities.append(city)\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Ports of Lading Received by top 6 US Ports',\n",
    "        showlegend = True,\n",
    "        geo = dict(\n",
    "            scope='world',\n",
    "            projection=dict( type='Mercator' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "            subunitwidth=1,\n",
    "            countrywidth=1,\n",
    "            subunitcolor=\"rgb(255, 255, 255)\",\n",
    "            countrycolor=\"rgb(255, 255, 255)\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=cities, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='top6_cities_heatmap' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the effect is still too much for that library.  There are 54,000 data points in the reduced subset.  Furthermore, when I zoom in on the max city (Shanghai), it looks like a large cluster of dots that're so uniformly spread out that it's pretty equivalent to just plotting a single marker of much greater area than the others (i.e., the approach I took when originally plotting these ports).  Clearly, this approach isn't going to generate the results I'm looking for.  The suggested adjustments they have are:\n",
    "\n",
    "    Woah there! Look at all those points! Due to browser limitations, the Plotly SVG drawing functions have a hard time graphing more than 500k data points for line charts, or 40k points for other types of charts. Here are some suggestions:\n",
    "    (1) Use the `plotly.graph_objs.Scattergl` trace object to generate a WebGl graph.\n",
    "    (2) Trying using the image API to return an image instead of a graph URL\n",
    "    (3) Use matplotlib\n",
    "    (4) See if you can create your visualization with fewer data points\n",
    "    \n",
    "Another possibility that I just hit upon is to try out [gmplot](https://github.com/vgm64/gmplot) a package that wraps the Google Maps API for Python, and says that it can overlay heatmaps fairly easily.  But it looks like an individual contributor's project, and the documentation for it is almost nonexistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, however, I'm more inclined to just attempt the choropleth approach.  I unzipped the shapefiles from gadm; there are 6 ('gadm_admX', where X is 0 through 5).  Their sizes vary between ~50MB and 500MB, but not monotonically increasing or decreasing.  The source page said the zip contained multiple levels of complexity and resolution, so I'm assuming that each file will represent the whole world, but at different levels of precision, rather than having some files only cover certain geographical regions.  I'll try the smallest file first, and see if I can get it to plot as polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "geo_s = {}\n",
    "for i in range(6):\n",
    "    geo_s['geodf'+str(i)] = gpd.read_file(paths['gadm28_adm{}_shp'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in geo_s.items():\n",
    "    print(k, type(v))\n",
    "    \n",
    "for k, v in geo_s.items():\n",
    "    print(k, v.shape)\n",
    "geo_s['geodf0'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, clearly my early supposition was incorrect.  Instead, the data for all countries *are* distributed across multiple shapefiles, and it's unclear whether there's any redundancy (i.e., subregions being proper subsets of regions covered as polygons elsewhere in the data set).  The titles of columns vary somewhat between shapefiles, but the fourth column (3 in zero-index-speak) is always the overall country's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shape_counts = geo_s['geodf5'].iloc[:, 3].value_counts()\n",
    "# shape_counts\n",
    "for k, v in geo_s.items():\n",
    "    print('{}:\\n{}\\n\\n'.format(k, v.iloc[:, 3].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in geo_s.items():\n",
    "    print('{}: {}'.format(k, v.iloc[:,3].unique().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'm starting to get it.  The `geodf0` is the most comprehensive, but just contains national boundaries.  `geodf1` appears to contain state/province boundaries, since the U.S. has 52, and the U.K. has 4 (I'm guessing England, Wales, Scotland, and Northern Ireland), but strangely Macedonia has >80 entries here.  Obviously this level of complexity of boundaries isn't the same for all countries.\n",
    "\n",
    "`geodf2` is getting too complicated: 3,148 entries for the US (probably county level).  Probably `geodf3` through 5 are just more extravagently labeled subsets of even fewer countries.  It's kind of funny that the last file only contains France and Rwanda, but has >10,000 entries for each.  How can you even split France into 36,000 polygons?\n",
    "\n",
    "Anyways, I definitely just `geodf0`, or perhaps `geodf1`.  The latter might be overkill, but they contain a tabulation of their areas; perhaps I could just keep the largest, and use national boundaries for everything else.\n",
    "\n",
    "It's a bit of a uphill slog, but I should check [these](https://plot.ly/python/county-choropleth/) [references](https://plot.ly/~jackp/18273.embed).\n",
    "\n",
    "As an alternative to plotly, it's [been pointed out](http://geopandas.org/mapping.html) that geopandas has a built-in module to output via matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "if ax:\n",
    "    ax.clear()\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(15, 10))\n",
    "ax = geo_s['geodf0'].plot(ax=ax)\n",
    "lims = plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that takes a long time to render, even with the fewest polygons possible (i.e., national boundaries).  I wonder if I can simply ignore any other than those that export to the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = top_6_df['foreign_port_of_lading'].apply(lambda x: x.split(sep=', ')[-1])\n",
    "countries.value_counts().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 82 instead of 256 countries.  Let's see if it speeds things up, and how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = pd.merge(geo_s['geodf0'], left_on='NAME_ENGLI', countries, how='right')\n",
    "geodf0 = geo_s['geodf0']\n",
    "subset = geodf0[geodf0['NAME_ENGLI'].isin(countries.tolist())]\n",
    "print(subset.shape)\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if ax:\n",
    "    ax.clear()\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(15, 10))\n",
    "ax = subset.plot(ax=ax)\n",
    "lims = plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf1 = geo_s['geodf1']\n",
    "geodf1.head()\n",
    "subset2 = geodf1[geodf1['NAME_0'].isin(countries.tolist())]\n",
    "print(subset2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh, that's still going to be too many to plot.  How about just trying to find the smaller polygons that contain a lat-lon point within the top_6_df list?  [Here's a write-up](https://gis.stackexchange.com/a/208574) about options for doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "point = geopandas.GeoDataFrame(top_6_df['lat_long'])\n",
    "poly  = geopandas.GeoDataFrame(geodf1['geometry'])\n",
    "from geopandas.tools import sjoin\n",
    "pointInPolys = sjoin(point, poly, how='left')\n",
    "grouped = pointInPolys.groupby('index_right')\n",
    "list(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopandas.tools import sjoin\n",
    "# countries = geopandas\n",
    "# from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "# points = Point(top_6_df['lat_long'])\n",
    "points = gpd.GeoDataFrame(top_6_df['lat_long'])\n",
    "# poly = \n",
    "crs = {'init': 'epsg:4326'}\n",
    "# gdf = gpd.GeoDataFrame(points, crs=crs, geometry=geometry)\n",
    "pointInPolys = gpd.tools.sjoin(points, geodf1['geometry'], how='left')\n",
    "grouped = pointInPolys.groupby('index_right')\n",
    "list(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(geopandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "#         colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = geo_s['geodf0']['geometry'].exterior,\n",
    "#         z = shp_brabant_lim['Color'].astype(float),\n",
    "        text = geo_s['geodf0'].iloc[:,3],\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Colorscale\")\n",
    "        ) ]\n",
    "\n",
    "\n",
    "layout = dict(\n",
    "    height=600,\n",
    "    autosize=True,\n",
    "    hovermode='closest'#,\n",
    "#     mapbox=dict(\n",
    "#         layers=[\n",
    "#             dict(\n",
    "#                 sourcetype = 'geojson',\n",
    "#                 source = 'DIRECTORY/shp_brabant_lim.json',\n",
    "#                 type = 'fill',\n",
    "#                 color = 'rgba(163,22,19,0.8)'\n",
    "#             )\n",
    "#         ],\n",
    "#         accesstoken=mapbox_access_token,\n",
    "#         bearing=0,\n",
    "#         center=dict(\n",
    "#             lat=51.6978,\n",
    "#             lon=-5.3037\n",
    "#         ),\n",
    "#         pitch=0,\n",
    "#         zoom=5.2,\n",
    "#         style='light'\n",
    "#     ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='National_boundaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(geo_s['geodf0'].iloc[0,-1])\n",
    "geo_s['geodf0'].iloc[0,-1].exterior.simplify(0.1)\n",
    "# geo_s['geodf0'].iloc[:,-1].exterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_s['geodf2'].iloc[:,3].value_counts()['United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Checking Weights\n",
    "\n",
    "Visualizing the number of records associated with different cities shipping to Seattle is cool, but the quantity and quality of goods might not be well represented just by number of containers.\n",
    "\n",
    "A more nuanced look would involve checking the total volume of goods coming from these places, in which case you'd probably want raw tonnage, which might be extractable from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in harmonized_weight_unit\n",
      "     Houston:  29,547 missing fields in  39,017 rows (75.7%).\n",
      "  Long_Beach: 100,242 missing fields in 116,721 rows (85.9%).\n",
      " Los_Angeles:  91,704 missing fields in 104,008 rows (88.2%).\n",
      "  NYC_Newark:  73,111 missing fields in  94,148 rows (77.7%).\n",
      "     Seattle:  19,172 missing fields in  27,098 rows (70.8%).\n",
      "      Tacoma:  27,512 missing fields in  33,920 rows (81.1%).\n",
      "\n",
      "Missing data in harmonized_value\n",
      "     Houston:  21,647 missing fields in  39,017 rows (55.5%).\n",
      "  Long_Beach:  90,026 missing fields in 116,721 rows (77.1%).\n",
      " Los_Angeles:  68,671 missing fields in 104,008 rows (66.0%).\n",
      "  NYC_Newark:  64,691 missing fields in  94,148 rows (68.7%).\n",
      "     Seattle:  10,662 missing fields in  27,098 rows (39.3%).\n",
      "      Tacoma:  13,825 missing fields in  33,920 rows (40.8%).\n",
      "\n",
      "Missing data in description_text\n",
      "     Houston:       0 missing fields in  39,017 rows ( 0.0%).\n",
      "  Long_Beach:       0 missing fields in 116,721 rows ( 0.0%).\n",
      " Los_Angeles:       0 missing fields in 104,008 rows ( 0.0%).\n",
      "  NYC_Newark:       0 missing fields in  94,148 rows ( 0.0%).\n",
      "     Seattle:       0 missing fields in  27,098 rows ( 0.0%).\n",
      "      Tacoma:       0 missing fields in  33,920 rows ( 0.0%).\n"
     ]
    }
   ],
   "source": [
    "print('Missing data in harmonized_weight_unit')\n",
    "for city, df in cities_2017_dfs.items():\n",
    "    missing_rows = df['harmonized_weight_unit'].isnull().sum()\n",
    "    total_rows = df.shape[0]\n",
    "    print('{:>12}: {:>7,} missing fields in {:>7,} rows ({:>4}%).'.format(\n",
    "        city, missing_rows, total_rows, np.round(missing_rows/total_rows*100, 1)))\n",
    "    \n",
    "print('\\nMissing data in harmonized_value')\n",
    "for city, df in cities_2017_dfs.items():\n",
    "    missing_rows = df['harmonized_weight'].isnull().sum()\n",
    "    total_rows = df.shape[0]\n",
    "    print('{:>12}: {:>7,} missing fields in {:>7,} rows ({:>4}%).'.format(\n",
    "        city, missing_rows, total_rows, np.round(missing_rows/total_rows*100, 1)))\n",
    "\n",
    "print('\\nMissing data in description_text')\n",
    "for city, df in cities_2017_dfs.items():\n",
    "    missing_rows = df['description_text'].isnull().sum()\n",
    "    total_rows = df.shape[0]\n",
    "    print('{:>12}: {:>7,} missing fields in {:>7,} rows ({:>4}%).'.format(\n",
    "        city, missing_rows, total_rows, np.round(missing_rows/total_rows*100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or they might not.  There are a lot of missing data for weight of containers; many others have a weight of zero listed.  Obviously the data entry is not standardized and strictly enforced.  But some of the extended text descriptions include some accounting of weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'SOFA & UPHOLSTERY PO418287/418285/418967 \"SH \"') \n",
      "\n",
      "(1000, 'WELDING WIRE') \n",
      "\n",
      "(2000, '55  CTN GDSM SOFA___TEL_ 1-650-685 8808 FAX_ 1-650-685 8898 S/C_US0002D2X CY TO CY 1X40_HQFCL') \n",
      "\n",
      "(3000, \"LADIES' 100% LINEN KNITTED  CA RDIGAN\") \n",
      "\n",
      "(4000, \"CHEMICAL WOOD PULP, SODA OR SULPHATE, OTHER T HAN D - 'THUNDER BAY' FULLY BLEACHED HARDWOOD KRAFT PULP\") \n",
      "\n",
      "(5000, 'ROLLS') \n",
      "\n",
      "(6000, 'TABLEWARE AND KITCHENWARE OF PLASTICS PLASTIC KITCHEN WARE 669317 GROUP PN3 +1 817-442-818 7') \n",
      "\n",
      "(7001, \"LOADED INTO 1 40'HIGH CUBE CONTAINER GRODAN / MINERAL WOOL NET WEIGHT :4952.0000 KGS HS-NO 68061000\") \n",
      "\n",
      "(8001, 'ORDER  0085910 ORIENTED STRAND BOARD SUMMARY ID  SUM2092 FREIGHT PREPAID HARMONIZATION COD ES  4410.12.00 NET UNIT  196.458 MSFT') \n",
      "\n",
      "(9001, 'CALCIUM STEARATE   HS CODE : 3 812.30   8255556   UDAB   SEA WAYBILL   FREIGHT COLLECT') \n",
      "\n",
      "(10001, 'DRIED LEGUMINOUS VEGETABLES, SHELLED, WHETHER OR N - X 100 LB BAGS CROP YEAR 2017: US CHIC KPEAS 2 X20 FCLCROP YEAR 2017: US CHICKPEAS 1 090 X 100 LB BAGS HS CODE: 0713202000 GROSS P OUNDS: 109,218.00 GROSS METRIC TON: 49.540 GR OSS KG: 49,540.00 NET POUNDS: 109,000.00 NET METRIC TON: 49.442 NET KG: 49,442.00 FREIGHTPREPAID DTHC PREPAID 14 FREE DEMURRAGE DAYS A T DESTINATION CAED: 01P233TCE45920171206801 A GENT AT DESTINATION DETAILS: MSC LANKA (PRIVA TE) LTD. 7TH FLOOR 193, DR DANISTER SILVA MAW ATHA COLOMBO 08 SRI LANKA PHONE+94 11 452 900 0 EMAILGENERAL=MSCLANKA.LK FAX+94 11 474 1771') \n",
      "\n",
      "(11001, 'WOOD SAWN OR CHIPPED LENGTHWISE, SLICED OR PE ELED, - DOUGLAS FIR KILN DRIED H.T. LUMBER (P SEUDOTSUGA MENZIESII) HEMLOCK KILN DRIED H.T. LUMBER (TSUGA HETEROPHYLLA) 45.8281 M3 19,42 1 FBM') \n",
      "\n",
      "(12001, 'HS CODE  020220 FROZEN BEEF CHUCK BONE IN CHU CK SHORT RIB   RIB BONE IN 123A SHORT RIB NET WT 50470.53 LBS NET WT 22893.28 KGS GRS WT 5 3375.09 LBS FREIGHT PREPAID MAINTAIN -18 DEG. CELSIUS   VENTS CLOSED CAED  01R054MC6323201 71202163') \n",
      "\n",
      "(13001, 'DRIED LEGUMINOUS VEGETABLES, SHELLED, WHETHER OR N - / POLY BAG CANADIAN LAIRD LENTILS, NU MBER TWO (2) CANADA - MID,  CROP YEAR 2017 NE T WEIGHT: 26.309MT') \n",
      "\n",
      "(14001, 'S/CS17ASC084 XTEL 022-23288588 FAX 022-23241555METAL FURNITURE PARTS SS SINKS METAL TABLE METAL CABINET UTILITY CART SS SOCKET OVERFLOW TUBESOAP DISPENSER FAUCET PARTS VALVE STOPCOCK FAUCET GDSM N/A STORE SUPPLY WAREHOUSE LLC (MR CODE ORISTO 3)') \n",
      "\n",
      "(15001, 'PAPER AND PAPERBOARD, COATED ON ONE OR BOTH S IDES - LWC PAPER (DEPENDOWEB) IN 59.2 GSM PAP EL LWC (DEPENDOWEB) DE 59.2 GRAMOS/M2 PKGS RO LLS   SIZE           MT CUBIC METERS -------- -  ---------- ------- ----------- ----------- ------------ 20       20     43.8  CM 25.632 22.194 ---------  ---------- ----------- ----------------------- TOTAL   20       20 25 .632    22.194 ---------  ---------- -------- --- ----------------------- FREIGHT PREPAID ( FLETE PAGADO) SUMMARY REPORTING NO. SUM 2214') \n",
      "\n",
      "(16001, 'GRASS SEEDS (AMBA / COCKSFOOT)') \n",
      "\n",
      "(17001, 'TUBING IN GAUGE METAL TUBING') \n",
      "\n",
      "(18002, 'POLYETHYLENE RESIN LOADED INTO 1 40CONTAINER(S) UNIVAL(TM) DMDA-6200 NT 7 HIGH DENSITY POLYETHYLENE RESIN 25 KG BAG 55 BAGS/PALLET TRADEMARKNET WEIGHT  24750.0000 KG HS-NO 3901209000') \n",
      "\n",
      "(19002, 'CLOTHES DRYER BUYER MODEL              QTY DL') \n",
      "\n",
      "(20002, 'ORGANIC GREEN COFFEE 02 LOTS CONVENTIONAL GREEN COFFEE 268 SACKS       18,492 KG 02 LOTS ORGANIC GREEN COFFEE GTN4401 08 SACKS 552 KG GTN4402 04 SACKS 276 KG') \n",
      "\n",
      "(21002, 'ONE EMPTY CONTAINER CLEARED AGAINST CUSTOMS') \n",
      "\n",
      "(22002, 'DRYER OF DRYER') \n",
      "\n",
      "(23002, 'COFFEE WASHED COSTA RICA ARABICA GREEN COFFEE MOVEMENT  FCL/FCL NET WEIGHT  18,975.00 KG. SC  S16ECE017 FREIGHT PAYABLE AL BASEL,SWITZERLAND BY COMMODITY SUPPLIES AG') \n",
      "\n",
      "(24002, 'AIR CONDITIONER') \n",
      "\n",
      "(25002, 'FRONT PLATE') \n",
      "\n",
      "(26002, 'PU NYLON RAINWEARPVC COTTON PANTPU POLYESTER RAINWEAR') \n",
      "\n",
      "(27002, 'WOOD SAWN OR CHIPPED LENGTHWISE, SLICED OR PE ELED, - WOOD, KILN DRIED, PRE-CUT') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for desc in cities_2017_dfs['Seattle'].loc[::1000, 'description_text'].iteritems():\n",
    "    print(desc, '\\n')\n",
    "\n",
    "# sea2017_df.loc[::10, 'description_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I count about 5 rows with weight info among those 28 that I sampled.  They're pretty variable, but sense could be made of them.  I wonder whether the rows that lack values in the dedicated weight column are more likely to contain weight information in the text description.\n",
    "\n",
    "But identifying the weight data from such a nonstandard format is tricky.  I might just need to pass a series of key terms like 'WT' and 'WEIGHT' with OR operators in between to a re pattern, or I may want to try sending the strings through some NLTK function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10662, 35), (8757, 35))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "prefixes = ['NET', 'GROSS', 'METRIC']\n",
    "triggers = ['WT', 'WEIGHT']\n",
    "units = ['POUNDS', 'LBS', 'TONS', 'KG', 'KGS']\n",
    "strings = [*prefixes, *triggers, *units]\n",
    "\n",
    "weight_words = ['NET', 'GROSS', 'METRIC', 'WT', 'WEIGHT', 'POUNDS',\n",
    "               'POUND', 'LBS', 'LB', 'TONS', 'TONNES', 'TON', 'KG', 'KGS']\n",
    "\n",
    "weight_words_string = '|'.join(x for x in weight_words)\n",
    "# pattern = '('+weight_words_string+'+[:?]\\s{1,2}\\d+)'\n",
    "# pattern = '(('+weight_words_string+')[:?]\\s{1,2}[\\d.,]+\\s{1,2}('+weight_words_string+'))'\n",
    "pattern = '('+weight_words_string+')'\n",
    "pattern = re.compile(pattern)\n",
    "pattern2 = re.compile('\\w+\\s\\d+')\n",
    "\n",
    "results = {}\n",
    "df = cities_2017_dfs['Seattle']\n",
    "df2 = df[df['harmonized_weight'].isnull()]# | df['harmonized_weight']==0]\n",
    "df3 = df[df['harmonized_weight'] == 0]\n",
    "df4 = pd.concat([df2, df3], axis=0)\n",
    "df5 = df4[~df4.index.duplicated(keep='first')]\n",
    "weight_matches = df5[df5['description_text'].str.match(pattern2)]\n",
    "# for i, v in df2.iteritems():\n",
    "#     if re.search(pattern, v):\n",
    "#         matches = re.search(pattern, v).groups()#[0]\n",
    "#         results[i] = matches\n",
    "# weight_matches = pd.Series(data=results)\n",
    "df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>trade_update_date</th>\n",
       "      <th>run_date</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>city</th>\n",
       "      <th>record_status_indicator</th>\n",
       "      <th>place_of_receipt</th>\n",
       "      <th>port_of_destination</th>\n",
       "      <th>...</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>description_text</th>\n",
       "      <th>harmonized_number</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>harmonized_weight</th>\n",
       "      <th>harmonized_weight_unit</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>containers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201712262236</td>\n",
       "      <td>2017-11-28T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Yantian, China</td>\n",
       "      <td>New</td>\n",
       "      <td>HAIPHONG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1243</td>\n",
       "      <td>APPAREL, MEN S WOVEN SHIRT, LADIES STRIPES SH</td>\n",
       "      <td>620520.0</td>\n",
       "      <td>241660.0</td>\n",
       "      <td>12083.0</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>22.556499</td>\n",
       "      <td>114.236875</td>\n",
       "      <td>(22.556499, 114.236875)</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>201712263368</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>BUSAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>EDTA-ACID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>35.179554</td>\n",
       "      <td>129.075642</td>\n",
       "      <td>(35.1795543, 129.0756416)</td>\n",
       "      <td>3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>201712263445</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Yangshan, China</td>\n",
       "      <td>New</td>\n",
       "      <td>SHANGHAI, SHANGHA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>640</td>\n",
       "      <td>OFFICE CHAIR</td>\n",
       "      <td>940130.0</td>\n",
       "      <td>215008.0</td>\n",
       "      <td>10750.0</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>24.466675</td>\n",
       "      <td>112.626495</td>\n",
       "      <td>(24.466675, 112.626495)</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>201712263500</td>\n",
       "      <td>2017-12-01T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Yangshan, China</td>\n",
       "      <td>New</td>\n",
       "      <td>SHANGHAI, SHANGHA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>NAILHEADS HTS CODE 940161 PO 12030814562 IF A</td>\n",
       "      <td>940161.0</td>\n",
       "      <td>57708.0</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>24.466675</td>\n",
       "      <td>112.626495</td>\n",
       "      <td>(24.466675, 112.626495)</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>201712263681</td>\n",
       "      <td>2017-12-15T00:00:00</td>\n",
       "      <td>2017-12-26T00:00:00</td>\n",
       "      <td>NEW YORK EXPRESS</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>Pusan, South Korea</td>\n",
       "      <td>New</td>\n",
       "      <td>TIANJIN XINGANG,</td>\n",
       "      <td>3512</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>EDTA-ACID S/C S17ASC112 MR CODE BLUBRE1 XFAX +...</td>\n",
       "      <td>292249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.179554</td>\n",
       "      <td>129.075642</td>\n",
       "      <td>(35.1795543, 129.0756416)</td>\n",
       "      <td>3240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      identifier    trade_update_date             run_date       vessel_name  \\\n",
       "13  201712262236  2017-11-28T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "29  201712263368  2017-12-01T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "30  201712263445  2017-12-01T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "31  201712263500  2017-12-01T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "32  201712263681  2017-12-15T00:00:00  2017-12-26T00:00:00  NEW YORK EXPRESS   \n",
       "\n",
       "       port_of_unlading estimated_arrival_date                city  \\\n",
       "13  Seattle, Washington    2017-12-21T00:00:00      Yantian, China   \n",
       "29  Seattle, Washington    2017-12-21T00:00:00  Pusan, South Korea   \n",
       "30  Seattle, Washington    2017-12-21T00:00:00     Yangshan, China   \n",
       "31  Seattle, Washington    2017-12-21T00:00:00     Yangshan, China   \n",
       "32  Seattle, Washington    2017-12-21T00:00:00  Pusan, South Korea   \n",
       "\n",
       "   record_status_indicator   place_of_receipt port_of_destination    ...      \\\n",
       "13                     New           HAIPHONG                 NaN    ...       \n",
       "29                     New              BUSAN                 NaN    ...       \n",
       "30                     New  SHANGHAI, SHANGHA                 NaN    ...       \n",
       "31                     New  SHANGHAI, SHANGHA                 NaN    ...       \n",
       "32                     New   TIANJIN XINGANG,                3512    ...       \n",
       "\n",
       "    piece_count                                   description_text  \\\n",
       "13         1243      APPAREL, MEN S WOVEN SHIRT, LADIES STRIPES SH   \n",
       "29           19                                          EDTA-ACID   \n",
       "30          640                                       OFFICE CHAIR   \n",
       "31           54      NAILHEADS HTS CODE 940161 PO 12030814562 IF A   \n",
       "32           19  EDTA-ACID S/C S17ASC112 MR CODE BLUBRE1 XFAX +...   \n",
       "\n",
       "   harmonized_number harmonized_value harmonized_weight  \\\n",
       "13          620520.0         241660.0           12083.0   \n",
       "29               NaN              0.0           19057.0   \n",
       "30          940130.0         215008.0           10750.0   \n",
       "31          940161.0          57708.0            2885.0   \n",
       "32          292249.0              0.0               0.0   \n",
       "\n",
       "   harmonized_weight_unit        lat        long                   lat_long  \\\n",
       "13              Kilograms  22.556499  114.236875    (22.556499, 114.236875)   \n",
       "29              Kilograms  35.179554  129.075642  (35.1795543, 129.0756416)   \n",
       "30              Kilograms  24.466675  112.626495    (24.466675, 112.626495)   \n",
       "31              Kilograms  24.466675  112.626495    (24.466675, 112.626495)   \n",
       "32                    NaN  35.179554  129.075642  (35.1795543, 129.0756416)   \n",
       "\n",
       "   containers  \n",
       "13        999  \n",
       "29       3240  \n",
       "30        721  \n",
       "31        721  \n",
       "32       3240  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight_matches[12000]\n",
    "# re.findall(r'[\\d,]+[.]?[\\d,]+', sea2017_df.loc[12000, 'description_text'])\n",
    "# sea2017_df.loc[12000, 'description_text']\n",
    "# weight_matches.head()\n",
    "weight_matches.shape\n",
    "# df5.shape\n",
    "df5.head()\n",
    "# cities_2017_dfs['Seattle'].head()\n",
    "df = cities_2017_dfs['Seattle']\n",
    "df[df.loc[:, 'harmonized_value'].notnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that list of words for capturing descriptions that might mention the weight of the cargo yields results that are a little underwhelming: only ~7,000 of the 19,000 rows that have no weight listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, v in empty['description_text'].iteritems():\n",
    "    if re.search(pattern, v):\n",
    "        matches = re.search(pattern, v).groups()#[0]\n",
    "        results[i] = matches\n",
    "empty_weight_matches = pd.Series(data=results)\n",
    "\n",
    "results = {}\n",
    "for i, v in zeroes['description_text'].iteritems():\n",
    "    if re.search(pattern, v):\n",
    "        matches = re.search(pattern, v).groups()#[0]\n",
    "        results[i] = matches\n",
    "zeroes_weight_matches = pd.Series(data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_weight_matches.shape, zeroes_weight_matches.shape\n",
    "empty_weight_matches.shape[0] + zeroes_weight_matches.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fewer than 5,000 of those rows happen to be in the records that lack a number in the 'harmonized_weight' column.  For now, I think it wouldn't be worth it to fine-tune the search strategy to recover those data.  Just plot the small subset of the data that include weight info.  But it wouldn't really be a great comparison to compare that plot to **all** of the data, since there may be a systematic underrepresentation of weight data from certain countries.  Actually, let's check for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = sea2017_df['lading_formatted']\\\n",
    ".value_counts().rename('all_cities')\n",
    "\n",
    "missing_weight_cities = pd.concat([\n",
    "    empty['lading_formatted'].value_counts(),\n",
    "    zeroes['lading_formatted'].value_counts()], axis=1)\n",
    "missing_sum = missing_weight_cities.sum(axis=1).sort_values(\n",
    "    ascending=False).rename('missing_weights')\n",
    "\n",
    "cities_weights = pd.concat([all_cities, missing_sum], axis=1)\n",
    "# cities_weights = missing_sum.join(all_cities)\n",
    "\n",
    "cities_weights['ratio_missing'] = \\\n",
    "cities_weights['missing_weights']/cities_weights['all_cities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_weights.sort_values(by='all_cities', ascending=False)[:10],\\\n",
    "cities_weights.sort_values(by='all_cities', ascending=False)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the proportion of missing data is fairly high throughout, but it does look like, overall, cities that ship to Seattle less overall also have a higer tendency to fall through the cracks in terms of reporting their weight.  Let's check real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_weights.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there's a weak negative correlation, meaning that cities with more records overall also have a slightly lower chance to be missing weight data.  It's not a tremendous trend, however, so it looks like summarizing the data set based on weight shouldn't be horribly misrepresentative of the total number of cities shipping.  So it might be worthwhile to graph them side-by-side after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea2017_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea2017_df['harmonized_weight_unit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('.*', 'blah')\n",
    "re.findall('[\\d.]+', '1234.56789')\n",
    "blahp = 'LBS|POUNDS|TONS'\n",
    "re.findall('(('+blahp+')?: [\\d.,]+)', 'POUNDS: 234.56')\n",
    "# re.findall(blahp+'\\d+', 'POUNDS234.56')\n",
    "# re.findall('((this|that)\\'s)', 'that\\'s')\n",
    "sea2017_df.loc[251, 'description_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8757 / sea2017_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a more interesting idea for now is to categorize *what* we're importing based on the description text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "One question that arises, in the case of Seattle, is why the overwhelming majority of shipments are coming from Vancouver, even though probably most of the ultimate origin of many imported goods is still China.  Vancouver is very close by, and may have a better port or otherwise have a different set of tariff/duties rules than the US, which would make it likely that foreign imports might shuttle through there.  It's a few years old, but [this article](https://www.joc.com/port-news/us-ports/us-importers-moving-more-containers-through-vancouver_20150618.html) states that many other US cities receive more imports lately from Vancouver, rather than Seattle or Tacoma, lending credence to the idea that Vancouver is a popular import point for foreign goods from overseas.  Actually, the article sites a labor dispute between longshoreman unions and ports in the PacNW as a driver for a temporary shift to Canada, which may have initiated a shift in traffic up North.  Still, the question as to why goods would pass through there on the way here seems a little puzzling.  A deeper dive into this issue could be interesting, or just too obscure to address adequately in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
